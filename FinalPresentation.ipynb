{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 align=\"center\">Capstone Project I</h1>\n",
    "\n",
    "# Introduction #\n",
    "**Problem Statement**: Classifying Amazon reviews based on customer ratings using NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 align=\"center\">Impact</h4>\n",
    "\n",
    "Reviews provide objective feedback to a product and are therefore inherently useful for consumers. These ratings are often summarized by a numerical rating, or the number of stars. Of course there is more value in the actual text itself than the quantified stars. And at times, the given rating does not truly convey the experience of the product â€“ the heart of the feedback is actually in the text itself. The goal therefore is to build a classifier that would understand the essence of a piece of review and assign it the most appropriate rating based on the meaning of the text.\n",
    "\n",
    "<h4 align=\"center\">Background</h4>\n",
    "\n",
    "Though product ratings on Amazon are aggregated from all the reviews by every customer, each individual rating is actually only an integer that ranges from one star to five stars. This reduces our predictions to discrete classes totaling five possibilities. Therefore what we'll have is a supervised, multi-class classifier with the actual review text as the core predictor.\n",
    "\n",
    "This study is an exploration of Natural Language Processing (NLP). The goal of predicting the star rating given a piece of text will take on different NLP topics including word embedding, topic modeling, and dimension reduction. From there, we'll arrive at a final dataframe and we'll be employing different machine learning techniques in order to come up with the best approach (i.e. most accurate estimator) for our classifier.\n",
    "\n",
    "<h4 align=\"center\" id=\"Datasets\">Datasets</h4>\n",
    "\n",
    "The [Amazon dataset](http://jmcauley.ucsd.edu/data/amazon/index.html) contains the customer reviews for all listed *Electronics* products spanning from May 1996 up to July 2014. There are a total of 1,689,188 reviews by a total of 192,403 customers on 63,001 unique products. The data dictionary is as follows:\n",
    "\n",
    "*  **asin** - Unique ID of the product being reviewed, *string*\n",
    "*  **helpful** - A list with two elements: the number of users that voted *helpful*, and the total number of users that voted on the review (including the *not helpful* votes), *list*\n",
    "*  **overall** - The reviewer's rating of the product, *int64*\n",
    "*  **reviewText** - The review text itself, *string*\n",
    "*  **reviewerID** - Unique ID of the reviewer, *string*\n",
    "*  **reviewerName** - Specified name of the reviewer, *string*\n",
    "*  **summary** - Headline summary of the review, *string*\n",
    "*  **unixReviewTime** - Unix Time of when the review was posted, *string*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `df` is created from the Amazon dataset. If the file has been downloaded then the dataset is loaded from the local file. Otherwise the file is accessed and extracted directly from the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2698442</td>\n",
       "      <td>2</td>\n",
       "      <td>An Amalgam</td>\n",
       "      <td>This book is an amalgam of bits and pieces and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2646715</td>\n",
       "      <td>5</td>\n",
       "      <td>Great!!!</td>\n",
       "      <td>Well, not much to say. If you saw the first se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2119569</td>\n",
       "      <td>2</td>\n",
       "      <td>Hit&amp;Miss</td>\n",
       "      <td>Babyface in his hey day always had a Cut that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>816322</td>\n",
       "      <td>5</td>\n",
       "      <td>Great Buy</td>\n",
       "      <td>This text is considered The Bible for any poli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1476562</td>\n",
       "      <td>3</td>\n",
       "      <td>What time is it anyway?</td>\n",
       "      <td>I thought the other reviews weren't serious ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17087</td>\n",
       "      <td>3</td>\n",
       "      <td>PRETTY FUNNY</td>\n",
       "      <td>GOOD,BUT UNREALISTIC.THE GUY JUST QUIT GOING T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1838666</td>\n",
       "      <td>5</td>\n",
       "      <td>Lacy J. Dalton</td>\n",
       "      <td>I saw Lacy on Bill Anderson's Country Reunion ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1442704</td>\n",
       "      <td>4</td>\n",
       "      <td>Great</td>\n",
       "      <td>Easy and enjoyable to watch. I would recommend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1114640</td>\n",
       "      <td>3</td>\n",
       "      <td>Be careful!!!!</td>\n",
       "      <td>I am a very advanced exerciser and have used t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1308335</td>\n",
       "      <td>5</td>\n",
       "      <td>the best book in the world!!!</td>\n",
       "      <td>Sahara special is one of the best books I have...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  0                              1  \\\n",
       "0     2698442  2                     An Amalgam   \n",
       "1     2646715  5                       Great!!!   \n",
       "2     2119569  2                       Hit&Miss   \n",
       "3      816322  5                      Great Buy   \n",
       "4     1476562  3        What time is it anyway?   \n",
       "5       17087  3                   PRETTY FUNNY   \n",
       "6     1838666  5                 Lacy J. Dalton   \n",
       "7     1442704  4                          Great   \n",
       "8     1114640  3                 Be careful!!!!   \n",
       "9     1308335  5  the best book in the world!!!   \n",
       "\n",
       "                                                   2  \n",
       "0  This book is an amalgam of bits and pieces and...  \n",
       "1  Well, not much to say. If you saw the first se...  \n",
       "2  Babyface in his hey day always had a Cut that ...  \n",
       "3  This text is considered The Bible for any poli...  \n",
       "4  I thought the other reviews weren't serious ab...  \n",
       "5  GOOD,BUT UNREALISTIC.THE GUY JUST QUIT GOING T...  \n",
       "6  I saw Lacy on Bill Anderson's Country Reunion ...  \n",
       "7  Easy and enjoyable to watch. I would recommend...  \n",
       "8  I am a very advanced exerciser and have used t...  \n",
       "9  Sahara special is one of the best books I have...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dataset=\"tarun.csv\"\n",
    "\n",
    "if os.path.isfile(dataset):\n",
    "    df = pd.read_csv(\"tarun.csv\")\n",
    "else:\n",
    "    url = r\"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Electronics_5.json.gz\"\n",
    "    df = pd.read_json(url, compression='gzip', lines=True)\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2698442</td>\n",
       "      <td>2</td>\n",
       "      <td>An Amalgam</td>\n",
       "      <td>This book is an amalgam of bits and pieces and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2646715</td>\n",
       "      <td>5</td>\n",
       "      <td>Great!!!</td>\n",
       "      <td>Well, not much to say. If you saw the first se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2119569</td>\n",
       "      <td>2</td>\n",
       "      <td>Hit&amp;Miss</td>\n",
       "      <td>Babyface in his hey day always had a Cut that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>816322</td>\n",
       "      <td>5</td>\n",
       "      <td>Great Buy</td>\n",
       "      <td>This text is considered The Bible for any poli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1476562</td>\n",
       "      <td>3</td>\n",
       "      <td>What time is it anyway?</td>\n",
       "      <td>I thought the other reviews weren't serious ab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  0                        1  \\\n",
       "0     2698442  2               An Amalgam   \n",
       "1     2646715  5                 Great!!!   \n",
       "2     2119569  2                 Hit&Miss   \n",
       "3      816322  5                Great Buy   \n",
       "4     1476562  3  What time is it anyway?   \n",
       "\n",
       "                                                   2  \n",
       "0  This book is an amalgam of bits and pieces and...  \n",
       "1  Well, not much to say. If you saw the first se...  \n",
       "2  Babyface in his hey day always had a Cut that ...  \n",
       "3  This text is considered The Bible for any poli...  \n",
       "4  I thought the other reviews weren't serious ab...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the `overall` and the `unixReviewTime` series are stored as integers. The rest are interpreted as strings (objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Unnamed: 0',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>An Amalgam</td>\n",
       "      <td>This book is an amalgam of bits and pieces and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Great!!!</td>\n",
       "      <td>Well, not much to say. If you saw the first se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hit&amp;Miss</td>\n",
       "      <td>Babyface in his hey day always had a Cut that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Great Buy</td>\n",
       "      <td>This text is considered The Bible for any poli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>What time is it anyway?</td>\n",
       "      <td>I thought the other reviews weren't serious ab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                        1  \\\n",
       "0  2               An Amalgam   \n",
       "1  5                 Great!!!   \n",
       "2  2                 Hit&Miss   \n",
       "3  5                Great Buy   \n",
       "4  3  What time is it anyway?   \n",
       "\n",
       "                                                   2  \n",
       "0  This book is an amalgam of bits and pieces and...  \n",
       "1  Well, not much to say. If you saw the first se...  \n",
       "2  Babyface in his hey day always had a Cut that ...  \n",
       "3  This text is considered The Bible for any poli...  \n",
       "4  I thought the other reviews weren't serious ab...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['overall','title','reviewText']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 187500 entries, 172280 to 280970\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   overall     187500 non-null  int64 \n",
      " 1   title       187496 non-null  object\n",
      " 2   reviewText  187500 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 5.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "df = df.sample(frac =.25)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>title</th>\n",
       "      <th>reviewText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172280</th>\n",
       "      <td>5</td>\n",
       "      <td>Excellent product</td>\n",
       "      <td>I bought this knee brace for my son who was co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342053</th>\n",
       "      <td>5</td>\n",
       "      <td>IF YOU LIKE MAPS AND COLOR PICTURES, THIS IS Y...</td>\n",
       "      <td>This is a great book for people who like lots ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345256</th>\n",
       "      <td>2</td>\n",
       "      <td>Would be great but BUGGY</td>\n",
       "      <td>This game has so much potential, but it is fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82597</th>\n",
       "      <td>5</td>\n",
       "      <td>Works as expected.</td>\n",
       "      <td>It's no frills, which is all I wanted. The cor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425572</th>\n",
       "      <td>1</td>\n",
       "      <td>horrible</td>\n",
       "      <td>horrible, absurd, cartoonish, ridiculous. do n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall                                              title  \\\n",
       "172280        5                                  Excellent product   \n",
       "342053        5  IF YOU LIKE MAPS AND COLOR PICTURES, THIS IS Y...   \n",
       "345256        2                           Would be great but BUGGY   \n",
       "82597         5                                 Works as expected.   \n",
       "425572        1                                           horrible   \n",
       "\n",
       "                                               reviewText  \n",
       "172280  I bought this knee brace for my son who was co...  \n",
       "342053  This is a great book for people who like lots ...  \n",
       "345256  This game has so much potential, but it is fil...  \n",
       "82597   It's no frills, which is all I wanted. The cor...  \n",
       "425572  horrible, absurd, cartoonish, ridiculous. do n...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Frequency - Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# function to plot most frequent terms\n",
    "def freq_words(x, terms = 30):\n",
    "  all_words = ' '.join([text for text in x])\n",
    "  all_words = all_words.split()\n",
    "\n",
    "  fdist = FreqDist(all_words)\n",
    "  words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())})\n",
    "\n",
    "  # selecting top 20 most frequent words\n",
    "  d = words_df.nlargest(columns=\"count\", n = terms) \n",
    "  plt.figure(figsize=(20,5))\n",
    "  ax = sns.barplot(data=d, x= \"word\", y = \"count\")\n",
    "  ax.set(ylabel = 'Count')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "freq_words(df['reviewText'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reviewTime` is dropped since the `unixReviewTime` series more accurately describes the time when each review was posted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I bought this knee brace for my son who was coming off of knee surgery(torn meniscus) He wore these knee pads from November to march, wrestling in 45 matches and daily practices. these pads were excellent in the way they cushioned his knee and protected him from any injury. the only down side was the cloth fabric sleeve's(holder) became worn out late in the season and it was required to be wrapped with tape around the top of the pad so it would stay up. i plan on purchasing another pair before next wrestling season.\n"
     ]
    }
   ],
   "source": [
    "print(df[\"reviewText\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each review is associated with a rating stored under the `overall` field. This serves as the quantified summary of a given review and will thus be used as the ground truth labels for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 2 1 4 3]\n"
     ]
    }
   ],
   "source": [
    "print(df.overall.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Pre-Processing ##\n",
    "We'll work with `reviewText` to prepare our model's final dataframe. The goal is to produce tokens for every document (i.e. every review). These documents will make up our corpora where we'll draw our vocabulary from.\n",
    "\n",
    "The following is a sample text in its original form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a great book for people who like lots of pictures and maps. It not only provides you with tips on travel (phonec calls,money ect) but it gives you insight into the different sections of the Loire. Each major attraction has one or more pages dedicated to it. The book really gives you a feel for what you will be seeing. It is one of the best guide books i have ever bought. I liked it so much i bought the one for Paris.\n"
     ]
    }
   ],
   "source": [
    "sample_review = df[\"reviewText\"].iloc[1]\n",
    "print(sample_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Entities ###\n",
    "Some special characters like the apostrophe (â€™) and the en dash (â€“) are expressed as a set of numbers prefixed by `&#` and suffixed by `;`. This is because the dataset was scraped from an HTML parser, and the dataset itself includes data that predated the universal UTF-8 standard.\n",
    "\n",
    "These *HTML Entities* can be decoded by importing the `html` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a great book for people who like lots of pictures and maps. It not only provides you with tips on travel (phonec calls,money ect) but it gives you insight into the different sections of the Loire. Each major attraction has one or more pages dedicated to it. The book really gives you a feel for what you will be seeing. It is one of the best guide books i have ever bought. I liked it so much i bought the one for Paris.\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "\n",
    "decoded_review = html.unescape(sample_review)\n",
    "print(decoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since punctuation marks do not add value in the way we'll perform NLP, all the HTML entities in the review texts can be dropped. The output series `preprocessed` is our `reviewText` but without the special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a great book for people who like lots of pictures and maps. It not only provides you with tips on travel (phonec calls,money ect) but it gives you insight into the different sections of the Loire. Each major attraction has one or more pages dedicated to it. The book really gives you a feel for what you will be seeing. It is one of the best guide books i have ever bought. I liked it so much i bought the one for Paris.\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"\\&\\#[0-9]+\\;\"\n",
    "\n",
    "df[\"preprocessed\"] = df[\"reviewText\"].str.replace(pat=pattern, repl=\"\", regex=True)\n",
    "\n",
    "print(df[\"preprocessed\"].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the root word ###\n",
    "How often a word is used is key information in natural language processing. It is therefore important to reduce words to their root form. An example would be the usage of the word \"*learn*\". If we differentiate this base form from a modified version like \"*learning*\" then we might lose relational context between two documents that have used either word.\n",
    "\n",
    "We'll be using Lemmatization to reduce tokens to their base word. This technique takes into account context similarity according to part-of-speech anatomy. Stemming is another common approach, although stemming only performs truncation and would not be able to reduce \"*taught*\" to \"*teach*\".\n",
    "\n",
    "We will be using the *WordNetLemmatizer* from the Natural Language Toolkit (or *NLTK*). Lemmatization only applies to each word but it is dependent on sentence structure to understand context. We therefore need to have part-of-speech tags associated with each word. Our output is derived from applying the `lemmatize_doc` function to our `preprocessed` column.\n",
    "\n",
    "The `lemmatize_doc` works as follows:\n",
    "* Each review is broken down into a list of sentences\n",
    "* Punctuations that only group words or separate sentences (hyphens therefore are excluded) are removed (replaced by whitespace) using RegEx\n",
    "* Every sentence is further broken down into words (tokens)\n",
    "\n",
    "Each of the sentences then becomes an ordered bag of words. Every word is then *tagged* to a part-of-speech. This word-tag tuple pair is then fed one at a time to the `lemmatize_word` function, which works as follows:\n",
    "* Only modifiable words â€“ nouns, verbs, adjectives, and adverbs â€“ can be reduced to roots\n",
    "* These words are lemmatized and appended to the `root` list\n",
    "* Words that are not modifiable are added as they are to the `root` list\n",
    "\n",
    "The output lists are linked together as a string using whitespace. In the end, each `preprocessed` review will retain its text form but with each word simplified as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This be a great book for people who like lot of picture and map It not only provide you with tip on travel phonec call money ect but it give you insight into the different section of the Loire Each major attraction have one or more page dedicate to it The book really give you a feel for what you will be see It be one of the best guide book i have ever buy I like it so much i buy the one for Paris\n",
      "Wall time: 19min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "#import nltk resources\n",
    "#resources = [\"wordnet\", \"stopwords\", \"punkt\", \\\n",
    "#             \"averaged_perceptron_tagger\", \"maxent_treebank_pos_tagger\"]\n",
    "resources =[\"wordnet\", \"stopwords\"]\n",
    "\n",
    "for resource in resources:\n",
    "    try:\n",
    "        nltk.data.find(\"tokenizers/\" + resource)\n",
    "    except LookupError:\n",
    "        nltk.download(resource)\n",
    "\n",
    "#create Lemmatizer object\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_word(tagged_token):\n",
    "    \"\"\" Returns lemmatized word given its tag\"\"\"\n",
    "    root = []\n",
    "    for token in tagged_token:\n",
    "        tag = token[1][0]\n",
    "        word = token[0]\n",
    "        if tag.startswith('J'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.ADJ))\n",
    "        elif tag.startswith('V'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.VERB))\n",
    "        elif tag.startswith('N'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.NOUN))\n",
    "        elif tag.startswith('R'):\n",
    "            root.append(lemma.lemmatize(word, wordnet.ADV))\n",
    "        else:          \n",
    "            root.append(word)\n",
    "    return root\n",
    "\n",
    "def lemmatize_doc(document):\n",
    "    \"\"\" Tags words then returns sentence with lemmatized words\"\"\"\n",
    "    lemmatized_list = []\n",
    "    tokenized_sent = sent_tokenize(document)\n",
    "    for sentence in tokenized_sent:\n",
    "        no_punctuation = re.sub(r\"[`'\\\",.!?()]\", \" \", sentence)\n",
    "        tokenized_word = word_tokenize(no_punctuation)\n",
    "        tagged_token = pos_tag(tokenized_word)\n",
    "        lemmatized = lemmatize_word(tagged_token)\n",
    "        lemmatized_list.extend(lemmatized)\n",
    "    return \" \".join(lemmatized_list)\n",
    "\n",
    "#apply our functions\n",
    "df[\"preprocessed\"] = df[\"preprocessed\"].apply(lambda row: lemmatize_doc(row))\n",
    "\n",
    "print(df[\"preprocessed\"].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Accents ###\n",
    "Each review is normalized from longform UTF-8 to ASCII encoding. This will remove accents in characters and ensure that words like \"*naÃ¯ve*\" will simply be interpreted as (and therefore not differentiated from) \"*naive*\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This be a great book for people who like lot of picture and map It not only provide you with tip on travel phonec call money ect but it give you insight into the different section of the Loire Each major attraction have one or more page dedicate to it The book really give you a feel for what you will be see It be one of the best guide book i have ever buy I like it so much i buy the one for Paris\n"
     ]
    }
   ],
   "source": [
    "from unicodedata import normalize\n",
    "\n",
    "remove_accent = lambda text: normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "df[\"preprocessed\"] = df[\"preprocessed\"].apply(remove_accent)\n",
    "\n",
    "print(df[\"preprocessed\"].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuations ###\n",
    "The `preprocessed` reviews are further cleaned by dropping punctuations. Using regular expressions, only whitespaces and alphanumeric characters are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This be a great book for people who like lot of picture and map It not only provide you with tip on travel phonec call money ect but it give you insight into the different section of the Loire Each major attraction have one or more page dedicate to it The book really give you a feel for what you will be see It be one of the best guide book i have ever buy I like it so much i buy the one for Paris\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"[^\\w\\s]\"\n",
    "\n",
    "df[\"preprocessed\"] = df[\"preprocessed\"].str.replace(pat=pattern, repl=\" \", regex=True)\n",
    "\n",
    "print(df[\"preprocessed\"].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to Lower Case ###\n",
    "Every letter is also converted to lower case. This makes it so that \"*iPhone*\" will not be distinguishable from \"*iphone*\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this be a great book for people who like lot of picture and map it not only provide you with tip on travel phonec call money ect but it give you insight into the different section of the loire each major attraction have one or more page dedicate to it the book really give you a feel for what you will be see it be one of the best guide book i have ever buy i like it so much i buy the one for paris\n"
     ]
    }
   ],
   "source": [
    "df[\"preprocessed\"] = df[\"preprocessed\"].str.lower()\n",
    "\n",
    "print(df[\"preprocessed\"].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words ###\n",
    "Stop words consist of the most commonly used words that include pronouns (e.g. *us*, *she*, *their*), articles (e.g. *the*), and prepositions (e.g. *under*, *from*, *off*). These words are not helpful in distinguishing a document from another and are therefore dropped.\n",
    "\n",
    "Note that the `stop_words` were stripped of punctuations just as what we have done to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample stop words: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'youre', 'youve', 'youll', 'youd', 'your', 'yours'] \n",
      "\n",
      "great book people like lot picture map provide tip travel phonec call money ect give insight different section loire major attraction one page dedicate book really give feel see one best guide book ever buy like much buy one paris\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "stop_words = [word.replace(\"\\'\", \"\") for word in stop_words]\n",
    "\n",
    "print(f\"sample stop words: {stop_words[:15]} \\n\")\n",
    "\n",
    "remove_stop_words = lambda row: \" \".join([token for token in row.split(\" \") \\\n",
    "                                          if token not in stop_words])\n",
    "df[\"preprocessed\"] = df[\"preprocessed\"].apply(remove_stop_words)\n",
    "\n",
    "print(df[\"preprocessed\"].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Extra Spaces ###\n",
    "Again, we make use of regular expressions to ensure we never get more than a single whitespace to separate words in our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "great book people like lot picture map provide tip travel phonec call money ect give insight different section loire major attraction one page dedicate book really give feel see one best guide book ever buy like much buy one paris\n"
     ]
    }
   ],
   "source": [
    "pattern = r\"[\\s]+\"\n",
    "\n",
    "df[\"preprocessed\"] = df[\"preprocessed\"].str.replace(pat=pattern, repl=\" \", regex=True)\n",
    "\n",
    "print(df[\"preprocessed\"].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "freq_words(df[\"preprocessed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization ##\n",
    "The entries for the `preprocessed` column are extracted to make up our *corpora*, which is simply a collection of all our documents. Each review is then transformed into an ordered list of words. This is the process of *tokenization* â€“ the document is broken down into individual words or tokens.\n",
    "\n",
    "Our tokenized sample review is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great', 'book', 'people', 'like', 'lot', 'picture', 'map', 'provide', 'tip', 'travel', 'phonec', 'call', 'money', 'ect', 'give', 'insight', 'different', 'section', 'loire', 'major', 'attraction', 'one', 'page', 'dedicate', 'book', 'really', 'give', 'feel', 'see', 'one', 'best', 'guide', 'book', 'ever', 'buy', 'like', 'much', 'buy', 'one', 'paris']\n"
     ]
    }
   ],
   "source": [
    "corpora = df[\"preprocessed\"].values\n",
    "tokenized = [corpus.split(\" \") for corpus in corpora]\n",
    "\n",
    "print(tokenized[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Modeling ##\n",
    "Since order of words matter in most NLP models, it is often helpful to group neighboring words that appear to convey one meaning as though they are a single word, like *smart TV*.\n",
    "\n",
    "To be considered a *phrase*, the number of times that two words should appear next to each other is set to at least `300`. The *threshold* then takes that minimum and compares it to the total number of token instances in the corpora. The higher the threshold, the more often two words must appear adjacent to be grouped into a phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "bi_gram = Phrases(tokenized, min_count=300, threshold=50)\n",
    "\n",
    "tri_gram = Phrases(bi_gram[tokenized], min_count=300, threshold=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams ###\n",
    "Unigrams are single pieces of tokens. The code below takes all the unique words from the entire corpora and prints a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thisresult', 'afterhe', 'dedicated', 'lawnchair', 'answersand', 'panicked', 'maimi', 'disintegrationcocteau', 'smomething', 'victems', 'beyondhis', 'snacks', 'loosend', 'inteligant', 'blurbings', '6amp', 'propagandai', 'giambi', 'guideline', 'sixshooter', 'woman2', 'oversensitive', 'portrayer', 'obscure', 'classring', 'hhr', 'arnulfo', 'technogly', 'siddeeq', 'drivil', 'badging', 'weaponary', 'muskateer', 'supprise', 'rtf', 'spelt', 'moralistic', 'cassablancas', 'geeeeez', 'austrianor', 'osmand', 'yurt', 'nist', 'another', 'brey', 'jenre', 'borde', 'whincing', 'conquered', 'yossef']\n"
     ]
    }
   ],
   "source": [
    "uni_gram_tokens = set([token for text in tokenized for token in text])\n",
    "uni_gram_tokens = set(filter(lambda x: x != \"\", uni_gram_tokens))\n",
    "\n",
    "print(list(uni_gram_tokens)[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams ###\n",
    "Bigrams are generated from using the *gensim* phraser. Only those that pass the `bi_gram` criteria are considered.\n",
    "\n",
    "The code below takes all the unique bigram phrases from the entire corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'find_good', 'people_say', 'take_long', 'would_want', 'know_would', 'wan_na', 'worth_price', '15_year', 'great_book', 'four_star', 'would_go', 'reason_give', '4_5', 'still_good', '2_day', 'never_see', 'end_book', 'short_story', 'tv_show', 'wish_could', 'well_think', 'give_3', 'find_way', 'many_different', 'blu_ray', 'one_time', 'lot_good', 'r_b', 'book_recommend', 'one_last', 'civil_war', 'make_one', 'many_book', 'product_work', 'anyone_else', 'everything_else', 'time_get', 'would_love', 'couple_year', 'good_read', 'piece_junk', 'enjoy_movie', 'great_cd', 'say_book', 'fit_well', '20_minute', 'best_song', 'least_one', 'like_say']\n"
     ]
    }
   ],
   "source": [
    "bigram_min = bi_gram.min_count\n",
    "\n",
    "bi_condition = lambda x: x[1] >= bigram_min\n",
    "\n",
    "bi_gram_tokens = dict(filter(bi_condition, bi_gram.vocab.items()))\n",
    "bi_gram_tokens = set([token.decode(\"utf-8\") \\\n",
    "                      for token in bi_gram_tokens])\n",
    "\n",
    "bi_grams_only = bi_gram_tokens.difference(uni_gram_tokens)\n",
    "print(list(bi_grams_only)[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams ###\n",
    "Trigrams are generated by applying another *gensim* phraser on top of a bigram phraser. Take for example the tokens *sd* and *card*. Because they appear often together enough, they become linked together as *sd_card*. In turn, if *sd_card* appears adjacent to the token *reader* in enough instances, then the `tri_gram` model would link them together as well to tokenize *sd_card_reader*.\n",
    "\n",
    "The code below takes all the unique trigram phrases from the entire corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['would_highly_recommend', 'highly_recommend_book']\n"
     ]
    }
   ],
   "source": [
    "trigram_min = tri_gram.min_count\n",
    "\n",
    "tri_condition = lambda x: x[1] >= trigram_min\n",
    "\n",
    "tri_gram_tokens = dict(filter(tri_condition, tri_gram.vocab.items()))\n",
    "tri_gram_tokens = set([token.decode(\"utf-8\") \\\n",
    "                       for token in tri_gram_tokens])\n",
    "\n",
    "tri_grams_only = tri_gram_tokens.difference(bi_gram_tokens)\n",
    "print(list(tri_grams_only)[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tri_gram` and `bi_gram` phrasers are applied to our `tokenized` corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tokenized = [Phraser(tri_gram)[Phraser(bi_gram)[i]] for i in tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single-character tokens are removed from every tokenized document. Our tokenized review, in its final form, is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['great', 'book', 'people', 'like', 'lot', 'picture', 'map', 'provide', 'tip', 'travel', 'phonec', 'call', 'money', 'ect', 'give', 'insight', 'different', 'section', 'loire', 'major', 'attraction', 'one', 'page', 'dedicate', 'book', 'really', 'give', 'feel', 'see', 'one', 'best', 'guide', 'book', 'ever', 'buy', 'like', 'much', 'buy', 'one', 'paris']\n"
     ]
    }
   ],
   "source": [
    "tokenized = [list(filter(lambda x: len(x) > 1, document)) \\\n",
    "             for document in tokenized]\n",
    "\n",
    "print(tokenized[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Vocabulary ##\n",
    "The `vocabulary` is the key-value pairs of all the unique tokens from every product review. Each token is assigned a lookup ID. The first 10 words in our dictionary are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 0, Token: 45\n",
      "ID: 1, Token: another\n",
      "ID: 2, Token: around\n",
      "ID: 3, Token: become\n",
      "ID: 4, Token: brace\n",
      "ID: 5, Token: buy\n",
      "ID: 6, Token: cloth\n",
      "ID: 7, Token: come\n",
      "ID: 8, Token: cushion\n",
      "ID: 9, Token: daily\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "vocabulary = Dictionary(tokenized)\n",
    "\n",
    "vocabulary_keys = list(vocabulary.token2id)[0:10]\n",
    "\n",
    "for key in vocabulary_keys:\n",
    "    print(f\"ID: {vocabulary.token2id[key]}, Token: {key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count-based Feature Engineering ##\n",
    "In order for a machine learning model to work with text input, the document must first be *vectorized*. This simply means that the input has to be converted into containers of numerical values.\n",
    "\n",
    "### Bag of Words Model ###\n",
    "The classical approach in expressing text as a set of features is getting the token frequency. Each entry to the dataframe is a document while each column corresponds to every unique token in the entire corpora. The row will identify how many times a word appears in the document. The `bow` model for the sample review is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: 45, Frequency: 1\n",
      "Word: another, Frequency: 1\n",
      "Word: around, Frequency: 1\n",
      "Word: become, Frequency: 1\n",
      "Word: brace, Frequency: 1\n",
      "Word: buy, Frequency: 1\n",
      "Word: cloth, Frequency: 1\n",
      "Word: come, Frequency: 1\n",
      "Word: cushion, Frequency: 1\n",
      "Word: daily, Frequency: 1\n",
      "Word: excellent, Frequency: 1\n",
      "Word: fabric, Frequency: 1\n",
      "Word: holder, Frequency: 1\n",
      "Word: injury, Frequency: 1\n",
      "Word: knee, Frequency: 4\n",
      "Word: late, Frequency: 1\n",
      "Word: march, Frequency: 1\n",
      "Word: match, Frequency: 1\n",
      "Word: meniscus, Frequency: 1\n",
      "Word: next, Frequency: 1\n",
      "Word: november, Frequency: 1\n",
      "Word: pad, Frequency: 3\n",
      "Word: pair, Frequency: 1\n",
      "Word: plan, Frequency: 1\n",
      "Word: practice, Frequency: 1\n",
      "Word: protect, Frequency: 1\n",
      "Word: purchase, Frequency: 1\n",
      "Word: require, Frequency: 1\n",
      "Word: season, Frequency: 2\n",
      "Word: side, Frequency: 1\n",
      "Word: sleeve, Frequency: 1\n",
      "Word: son, Frequency: 1\n",
      "Word: stay, Frequency: 1\n",
      "Word: surgery, Frequency: 1\n",
      "Word: tape, Frequency: 1\n",
      "Word: top, Frequency: 1\n",
      "Word: torn, Frequency: 1\n",
      "Word: way, Frequency: 1\n",
      "Word: wear, Frequency: 1\n",
      "Word: worn, Frequency: 1\n",
      "Word: would, Frequency: 1\n",
      "Word: wrap, Frequency: 1\n",
      "Word: wrestling, Frequency: 2\n"
     ]
    }
   ],
   "source": [
    "bow = [vocabulary.doc2bow(doc) for doc in tokenized]\n",
    "\n",
    "for idx, freq in bow[0]:\n",
    "    print(f\"Word: {vocabulary.get(idx)}, Frequency: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Model ###\n",
    "The Term Frequency-Inverse Document Frequency (*TF-IDF*) approach assigns continuous values instead of simple integers for the token frequency. Words that appear frequently overall tend to not establish saliency in a document, and are thus weighted lower. Words that are unique to some documents tend to help distinguish it from the rest and are thus weighted higher. The `tfidf` weighting is based on our `bow` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: 45, Weight: 0.125\n",
      "Word: another, Weight: 0.058\n",
      "Word: around, Weight: 0.065\n",
      "Word: become, Weight: 0.075\n",
      "Word: brace, Weight: 0.158\n",
      "Word: buy, Weight: 0.036\n",
      "Word: cloth, Weight: 0.133\n",
      "Word: come, Weight: 0.048\n",
      "Word: cushion, Weight: 0.137\n",
      "Word: daily, Weight: 0.108\n",
      "Word: excellent, Weight: 0.070\n",
      "Word: fabric, Weight: 0.124\n",
      "Word: holder, Weight: 0.122\n",
      "Word: injury, Weight: 0.146\n",
      "Word: knee, Weight: 0.541\n",
      "Word: late, Weight: 0.087\n",
      "Word: march, Weight: 0.133\n",
      "Word: match, Weight: 0.100\n",
      "Word: meniscus, Weight: 0.233\n",
      "Word: next, Weight: 0.072\n",
      "Word: november, Weight: 0.143\n",
      "Word: pad, Weight: 0.324\n",
      "Word: pair, Weight: 0.102\n",
      "Word: plan, Weight: 0.091\n",
      "Word: practice, Weight: 0.102\n",
      "Word: protect, Weight: 0.115\n",
      "Word: purchase, Weight: 0.056\n",
      "Word: require, Weight: 0.094\n",
      "Word: season, Weight: 0.187\n",
      "Word: side, Weight: 0.076\n",
      "Word: sleeve, Weight: 0.133\n",
      "Word: son, Weight: 0.079\n",
      "Word: stay, Weight: 0.080\n",
      "Word: surgery, Weight: 0.144\n",
      "Word: tape, Weight: 0.096\n",
      "Word: top, Weight: 0.076\n",
      "Word: torn, Weight: 0.151\n",
      "Word: way, Weight: 0.049\n",
      "Word: wear, Weight: 0.085\n",
      "Word: worn, Weight: 0.137\n",
      "Word: would, Weight: 0.033\n",
      "Word: wrap, Weight: 0.112\n",
      "Word: wrestling, Weight: 0.315\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "tfidf = TfidfModel(bow)\n",
    "\n",
    "for idx, weight in tfidf[bow[0]]:\n",
    "    print(f\"Word: {vocabulary.get(idx)}, Weight: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding for Feature Engineering ##\n",
    "The downside of count-based techniques is that without regard to word sequence and sentence structure, the semantics get lost. The *Word2Vec* technique, on the other hand, actually embeds meaning in vectors by quantifying how often a word appears within the vicinity of a given set of other words.\n",
    "\n",
    "A context window the span of `context_size` slides across every document one token at a time. In each step, the center word is described by its adjacent words and the probability that the token appears together with the others is expressed in `feature_size` dimensions. Since the minimum word requirement is set to `1`, every token in the corpora is embedded in the *Word2Vec* model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "feature_size = 100\n",
    "context_size = 20\n",
    "min_word = 1\n",
    "\n",
    "word_vec= word2vec.Word2Vec(tokenized, size=feature_size, \\\n",
    "                            window=context_size, min_count=min_word, \\\n",
    "                            iter=50, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Dataframe ##\n",
    "The goal is to have a dataframe with observations corresponding to the product reviews. The `word_vec` model is used to gather all the unique tokens in the corpora. This enables us to generate the `word_vec_df` which makes use of the dimensions as the features of every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>buy</th>\n",
       "      <td>-0.391614</td>\n",
       "      <td>0.214361</td>\n",
       "      <td>-0.297795</td>\n",
       "      <td>-1.877798</td>\n",
       "      <td>-4.291623</td>\n",
       "      <td>-0.978395</td>\n",
       "      <td>-0.937741</td>\n",
       "      <td>0.463861</td>\n",
       "      <td>-2.089837</td>\n",
       "      <td>-2.478668</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.204032</td>\n",
       "      <td>1.829290</td>\n",
       "      <td>-3.479812</td>\n",
       "      <td>-0.199290</td>\n",
       "      <td>0.338583</td>\n",
       "      <td>-2.067757</td>\n",
       "      <td>4.804310</td>\n",
       "      <td>-2.475281</td>\n",
       "      <td>-0.648905</td>\n",
       "      <td>2.271200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knee</th>\n",
       "      <td>1.481127</td>\n",
       "      <td>-1.978232</td>\n",
       "      <td>-5.700877</td>\n",
       "      <td>1.706983</td>\n",
       "      <td>0.162963</td>\n",
       "      <td>1.898773</td>\n",
       "      <td>-2.902293</td>\n",
       "      <td>0.457745</td>\n",
       "      <td>2.619406</td>\n",
       "      <td>1.072991</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.497428</td>\n",
       "      <td>-1.427017</td>\n",
       "      <td>-2.086283</td>\n",
       "      <td>1.251740</td>\n",
       "      <td>-3.486538</td>\n",
       "      <td>-1.234355</td>\n",
       "      <td>2.849387</td>\n",
       "      <td>1.263205</td>\n",
       "      <td>-2.140125</td>\n",
       "      <td>1.255013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brace</th>\n",
       "      <td>2.338521</td>\n",
       "      <td>-1.144466</td>\n",
       "      <td>-4.394060</td>\n",
       "      <td>-1.000188</td>\n",
       "      <td>-0.073849</td>\n",
       "      <td>0.861388</td>\n",
       "      <td>-4.325711</td>\n",
       "      <td>-1.433344</td>\n",
       "      <td>3.800290</td>\n",
       "      <td>-1.003659</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.449026</td>\n",
       "      <td>-0.469251</td>\n",
       "      <td>-3.572661</td>\n",
       "      <td>0.682496</td>\n",
       "      <td>-5.588127</td>\n",
       "      <td>1.907107</td>\n",
       "      <td>2.072536</td>\n",
       "      <td>-1.930896</td>\n",
       "      <td>0.755664</td>\n",
       "      <td>-1.787867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>son</th>\n",
       "      <td>-0.784669</td>\n",
       "      <td>-4.240441</td>\n",
       "      <td>-0.102267</td>\n",
       "      <td>2.689479</td>\n",
       "      <td>-0.944507</td>\n",
       "      <td>1.638644</td>\n",
       "      <td>-0.295057</td>\n",
       "      <td>-1.015112</td>\n",
       "      <td>-6.617234</td>\n",
       "      <td>0.473892</td>\n",
       "      <td>...</td>\n",
       "      <td>1.646912</td>\n",
       "      <td>-7.015779</td>\n",
       "      <td>-2.300763</td>\n",
       "      <td>2.594702</td>\n",
       "      <td>-2.190922</td>\n",
       "      <td>5.335032</td>\n",
       "      <td>-0.068625</td>\n",
       "      <td>1.043740</td>\n",
       "      <td>2.853506</td>\n",
       "      <td>-0.789644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>-0.860967</td>\n",
       "      <td>-2.615756</td>\n",
       "      <td>2.044501</td>\n",
       "      <td>0.057479</td>\n",
       "      <td>-1.821402</td>\n",
       "      <td>-0.822883</td>\n",
       "      <td>-2.908396</td>\n",
       "      <td>1.012116</td>\n",
       "      <td>-2.599405</td>\n",
       "      <td>-1.368737</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.183804</td>\n",
       "      <td>-0.565112</td>\n",
       "      <td>-0.204980</td>\n",
       "      <td>-5.581378</td>\n",
       "      <td>3.131901</td>\n",
       "      <td>-1.471195</td>\n",
       "      <td>1.289782</td>\n",
       "      <td>-2.469403</td>\n",
       "      <td>-0.555767</td>\n",
       "      <td>-1.916383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "buy   -0.391614  0.214361 -0.297795 -1.877798 -4.291623 -0.978395 -0.937741   \n",
       "knee   1.481127 -1.978232 -5.700877  1.706983  0.162963  1.898773 -2.902293   \n",
       "brace  2.338521 -1.144466 -4.394060 -1.000188 -0.073849  0.861388 -4.325711   \n",
       "son   -0.784669 -4.240441 -0.102267  2.689479 -0.944507  1.638644 -0.295057   \n",
       "come  -0.860967 -2.615756  2.044501  0.057479 -1.821402 -0.822883 -2.908396   \n",
       "\n",
       "             7         8         9   ...        90        91        92  \\\n",
       "buy    0.463861 -2.089837 -2.478668  ... -2.204032  1.829290 -3.479812   \n",
       "knee   0.457745  2.619406  1.072991  ... -2.497428 -1.427017 -2.086283   \n",
       "brace -1.433344  3.800290 -1.003659  ... -1.449026 -0.469251 -3.572661   \n",
       "son   -1.015112 -6.617234  0.473892  ...  1.646912 -7.015779 -2.300763   \n",
       "come   1.012116 -2.599405 -1.368737  ... -0.183804 -0.565112 -0.204980   \n",
       "\n",
       "             93        94        95        96        97        98        99  \n",
       "buy   -0.199290  0.338583 -2.067757  4.804310 -2.475281 -0.648905  2.271200  \n",
       "knee   1.251740 -3.486538 -1.234355  2.849387  1.263205 -2.140125  1.255013  \n",
       "brace  0.682496 -5.588127  1.907107  2.072536 -1.930896  0.755664 -1.787867  \n",
       "son    2.594702 -2.190922  5.335032 -0.068625  1.043740  2.853506 -0.789644  \n",
       "come  -5.581378  3.131901 -1.471195  1.289782 -2.469403 -0.555767 -1.916383  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_vec_unpack = [(word, idx.index) for word, idx in \\\n",
    "                   word_vec.wv.vocab.items()]\n",
    "\n",
    "tokens, indexes = zip(*word_vec_unpack)\n",
    "\n",
    "word_vec_df = pd.DataFrame(word_vec.wv.syn0[indexes, :], index=tokens)\n",
    "\n",
    "display(word_vec_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `word_vec_df` is sliced by the words that appear in a given `tokenized` review and the mean along every dimension is taken. The resulting `model_array` shape is therefore the word count on *axis 0* and the number of dimensions on *axis 1*. This singularizes multiple word embeddings into one observation for each review.\n",
    "\n",
    "If multiple occurrences of a word occurs in a review, then this only emphasizes the token since the row is pulled towards the values of the vectors of that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenized_array = np.array(tokenized)\n",
    "\n",
    "model_array = np.array([word_vec_df.loc[doc].mean(axis=0) for doc in tokenized_array])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every document is provided the ground truth label by imposing its `overall` rating. This completes our finalized `model_df` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.060511</td>\n",
       "      <td>-0.188223</td>\n",
       "      <td>-1.115128</td>\n",
       "      <td>0.362478</td>\n",
       "      <td>0.119871</td>\n",
       "      <td>0.998860</td>\n",
       "      <td>-1.463232</td>\n",
       "      <td>-0.143590</td>\n",
       "      <td>-0.202608</td>\n",
       "      <td>0.693012</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.900333</td>\n",
       "      <td>-1.395217</td>\n",
       "      <td>0.004696</td>\n",
       "      <td>-0.846067</td>\n",
       "      <td>0.007666</td>\n",
       "      <td>0.698607</td>\n",
       "      <td>-0.440302</td>\n",
       "      <td>0.402417</td>\n",
       "      <td>0.177233</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.891368</td>\n",
       "      <td>-0.790383</td>\n",
       "      <td>-1.417212</td>\n",
       "      <td>0.748750</td>\n",
       "      <td>-1.663503</td>\n",
       "      <td>-0.814701</td>\n",
       "      <td>-0.004924</td>\n",
       "      <td>0.921140</td>\n",
       "      <td>-0.263571</td>\n",
       "      <td>-1.453477</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.612079</td>\n",
       "      <td>0.629601</td>\n",
       "      <td>0.779798</td>\n",
       "      <td>0.906917</td>\n",
       "      <td>0.025658</td>\n",
       "      <td>0.577646</td>\n",
       "      <td>0.190882</td>\n",
       "      <td>1.190661</td>\n",
       "      <td>0.824445</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.992819</td>\n",
       "      <td>-0.578294</td>\n",
       "      <td>-1.753037</td>\n",
       "      <td>-0.246599</td>\n",
       "      <td>0.621295</td>\n",
       "      <td>0.340919</td>\n",
       "      <td>0.657904</td>\n",
       "      <td>0.068378</td>\n",
       "      <td>-0.548838</td>\n",
       "      <td>-0.889405</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298908</td>\n",
       "      <td>0.132332</td>\n",
       "      <td>0.660255</td>\n",
       "      <td>-0.103366</td>\n",
       "      <td>-0.825894</td>\n",
       "      <td>0.063889</td>\n",
       "      <td>0.006260</td>\n",
       "      <td>-0.778452</td>\n",
       "      <td>-0.055913</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.047363</td>\n",
       "      <td>0.841540</td>\n",
       "      <td>-1.503287</td>\n",
       "      <td>0.574135</td>\n",
       "      <td>1.264495</td>\n",
       "      <td>0.401423</td>\n",
       "      <td>-0.114395</td>\n",
       "      <td>-0.111660</td>\n",
       "      <td>0.992983</td>\n",
       "      <td>-1.897826</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.561008</td>\n",
       "      <td>-1.178640</td>\n",
       "      <td>0.296056</td>\n",
       "      <td>1.037051</td>\n",
       "      <td>-1.279606</td>\n",
       "      <td>0.767893</td>\n",
       "      <td>-0.765034</td>\n",
       "      <td>2.213472</td>\n",
       "      <td>1.210326</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.256347</td>\n",
       "      <td>-0.637179</td>\n",
       "      <td>-1.493984</td>\n",
       "      <td>1.059257</td>\n",
       "      <td>-0.970549</td>\n",
       "      <td>-1.404447</td>\n",
       "      <td>-0.472565</td>\n",
       "      <td>1.561990</td>\n",
       "      <td>-2.069466</td>\n",
       "      <td>0.201867</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.325816</td>\n",
       "      <td>0.038941</td>\n",
       "      <td>-0.327367</td>\n",
       "      <td>1.010343</td>\n",
       "      <td>-0.590240</td>\n",
       "      <td>0.791885</td>\n",
       "      <td>1.099817</td>\n",
       "      <td>-0.458528</td>\n",
       "      <td>0.494933</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  1.060511 -0.188223 -1.115128  0.362478  0.119871  0.998860 -1.463232   \n",
       "1  0.891368 -0.790383 -1.417212  0.748750 -1.663503 -0.814701 -0.004924   \n",
       "2  0.992819 -0.578294 -1.753037 -0.246599  0.621295  0.340919  0.657904   \n",
       "3 -0.047363  0.841540 -1.503287  0.574135  1.264495  0.401423 -0.114395   \n",
       "4  1.256347 -0.637179 -1.493984  1.059257 -0.970549 -1.404447 -0.472565   \n",
       "\n",
       "          7         8         9  ...        91        92        93        94  \\\n",
       "0 -0.143590 -0.202608  0.693012  ... -1.900333 -1.395217  0.004696 -0.846067   \n",
       "1  0.921140 -0.263571 -1.453477  ... -0.612079  0.629601  0.779798  0.906917   \n",
       "2  0.068378 -0.548838 -0.889405  ... -0.298908  0.132332  0.660255 -0.103366   \n",
       "3 -0.111660  0.992983 -1.897826  ... -0.561008 -1.178640  0.296056  1.037051   \n",
       "4  1.561990 -2.069466  0.201867  ... -1.325816  0.038941 -0.327367  1.010343   \n",
       "\n",
       "         95        96        97        98        99  label  \n",
       "0  0.007666  0.698607 -0.440302  0.402417  0.177233    NaN  \n",
       "1  0.025658  0.577646  0.190882  1.190661  0.824445    NaN  \n",
       "2 -0.825894  0.063889  0.006260 -0.778452 -0.055913    NaN  \n",
       "3 -1.279606  0.767893 -0.765034  2.213472  1.210326    NaN  \n",
       "4 -0.590240  0.791885  1.099817 -0.458528  0.494933    NaN  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_df = pd.DataFrame(model_array)\n",
    "model_df[\"label\"] = df[\"overall\"]\n",
    "\n",
    "display(model_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis ##\n",
    "Principal Component Analysis (*PCA*) is a dimensionality reduction technique that we can use on our `model_df` to reduce its 100 dimensions to just two dimensions. This will help visualize if there is a clear decision boundary along the five `overall` rating classifications. The more datapoints belonging to the same class are clustered together, the higher the likelihood that our machine learning model is simpler and more effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>title</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>Positively_Rated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172280</th>\n",
       "      <td>5</td>\n",
       "      <td>Excellent product</td>\n",
       "      <td>I bought this knee brace for my son who was co...</td>\n",
       "      <td>buy knee brace son come knee surgery torn meni...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342053</th>\n",
       "      <td>5</td>\n",
       "      <td>IF YOU LIKE MAPS AND COLOR PICTURES, THIS IS Y...</td>\n",
       "      <td>This is a great book for people who like lots ...</td>\n",
       "      <td>great book people like lot picture map provide...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345256</th>\n",
       "      <td>2</td>\n",
       "      <td>Would be great but BUGGY</td>\n",
       "      <td>This game has so much potential, but it is fil...</td>\n",
       "      <td>game much potential fill fatal bug patch yet a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82597</th>\n",
       "      <td>5</td>\n",
       "      <td>Works as expected.</td>\n",
       "      <td>It's no frills, which is all I wanted. The cor...</td>\n",
       "      <td>frill want cord long enough conveniently place...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425572</th>\n",
       "      <td>1</td>\n",
       "      <td>horrible</td>\n",
       "      <td>horrible, absurd, cartoonish, ridiculous. do n...</td>\n",
       "      <td>horrible absurd cartoonish ridiculous waste tw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681236</th>\n",
       "      <td>4</td>\n",
       "      <td>bubble gum music lives!</td>\n",
       "      <td>Hail to bobby sherman! Now HE WAS THE ULTIMATE...</td>\n",
       "      <td>hail bobby sherman ultimate pin poster good lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473291</th>\n",
       "      <td>4</td>\n",
       "      <td>They're blind and deaf or liars or idiots or a...</td>\n",
       "      <td>THE AUDIO AND VIDEO OF THIS FLICK ARE JUST FIN...</td>\n",
       "      <td>audio video flick fine find tame see ever shoc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546197</th>\n",
       "      <td>5</td>\n",
       "      <td>Best Marley kids' album yet</td>\n",
       "      <td>This cd is great. It is both smoothe and relax...</td>\n",
       "      <td>cd great smoothe relaxing intense sensitive ev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607499</th>\n",
       "      <td>4</td>\n",
       "      <td>Attactor Factor</td>\n",
       "      <td>This Book in incredible for keeping you on tra...</td>\n",
       "      <td>book incredible keep track focus positive thin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158807</th>\n",
       "      <td>5</td>\n",
       "      <td>Trance at its best?</td>\n",
       "      <td>Here we have two guys from Sweden doing what t...</td>\n",
       "      <td>two guy sweden best first worldwide release an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall                                              title  \\\n",
       "172280        5                                  Excellent product   \n",
       "342053        5  IF YOU LIKE MAPS AND COLOR PICTURES, THIS IS Y...   \n",
       "345256        2                           Would be great but BUGGY   \n",
       "82597         5                                 Works as expected.   \n",
       "425572        1                                           horrible   \n",
       "...         ...                                                ...   \n",
       "681236        4                            bubble gum music lives!   \n",
       "473291        4  They're blind and deaf or liars or idiots or a...   \n",
       "546197        5                        Best Marley kids' album yet   \n",
       "607499        4                                    Attactor Factor   \n",
       "158807        5                                Trance at its best?   \n",
       "\n",
       "                                               reviewText  \\\n",
       "172280  I bought this knee brace for my son who was co...   \n",
       "342053  This is a great book for people who like lots ...   \n",
       "345256  This game has so much potential, but it is fil...   \n",
       "82597   It's no frills, which is all I wanted. The cor...   \n",
       "425572  horrible, absurd, cartoonish, ridiculous. do n...   \n",
       "...                                                   ...   \n",
       "681236  Hail to bobby sherman! Now HE WAS THE ULTIMATE...   \n",
       "473291  THE AUDIO AND VIDEO OF THIS FLICK ARE JUST FIN...   \n",
       "546197  This cd is great. It is both smoothe and relax...   \n",
       "607499  This Book in incredible for keeping you on tra...   \n",
       "158807  Here we have two guys from Sweden doing what t...   \n",
       "\n",
       "                                             preprocessed  Positively_Rated  \n",
       "172280  buy knee brace son come knee surgery torn meni...                 1  \n",
       "342053  great book people like lot picture map provide...                 1  \n",
       "345256  game much potential fill fatal bug patch yet a...                 0  \n",
       "82597   frill want cord long enough conveniently place...                 1  \n",
       "425572  horrible absurd cartoonish ridiculous waste tw...                 0  \n",
       "...                                                   ...               ...  \n",
       "681236  hail bobby sherman ultimate pin poster good lo...                 1  \n",
       "473291  audio video flick fine find tame see ever shoc...                 1  \n",
       "546197  cd great smoothe relaxing intense sensitive ev...                 1  \n",
       "607499  book incredible keep track focus positive thin...                 1  \n",
       "158807  two guy sweden best first worldwide release an...                 1  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df = df[df['overall']!= 3]\n",
    "df['Positively_Rated'] = np.where(df['overall']>3, 1, 0)\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>Total count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positively_Rated</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>74854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0             Total count\n",
       "Positively_Rated             \n",
       "0                       75011\n",
       "1                       74854"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(index = df['Positively_Rated'], columns=\"Total count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['reviewText'], df['Positively_Rated'], test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train first entry:\n",
      "\n",
      " \"Swedish Exodus\" is an indespensible reference for Swedish researchers. In these pages one can find out why so many Swedes decided to come to America, how they mananged it, where they emigrated to in America, how many came during each year of the mass migration, how many remigrated each year, how many came from each major Swedish district, and how they did in America. It even outlines the consequences of the emigration on Sweden. It is nicely indexed, contains a useful bibliography, and is illustrated with photos and drawings.\n",
      "\n",
      "\n",
      "X_train shape:  (119892,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train first entry:\\n\\n', X_train.iloc[2])\n",
    "print('\\n\\nX_train shape: ', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '0stars',\n",
       " '10rock',\n",
       " '130mbs',\n",
       " '1600g',\n",
       " '1860',\n",
       " '1997s',\n",
       " '20dbgain',\n",
       " '2500mah',\n",
       " '2inches',\n",
       " '32',\n",
       " '38oz',\n",
       " '4137',\n",
       " '4ever',\n",
       " '530platinum',\n",
       " '5vdc',\n",
       " '6980',\n",
       " '77121',\n",
       " '8830',\n",
       " '970hd',\n",
       " '_the',\n",
       " 'abdiel',\n",
       " 'absalom',\n",
       " 'accents',\n",
       " 'accustome',\n",
       " 'acrobats',\n",
       " 'adaptable',\n",
       " 'adjustible',\n",
       " 'advaita',\n",
       " 'aewsome',\n",
       " 'afternoons',\n",
       " 'agregarlos',\n",
       " 'airforce',\n",
       " 'alaska',\n",
       " 'alford',\n",
       " 'allister',\n",
       " 'alround',\n",
       " 'amang',\n",
       " 'america',\n",
       " 'amplio',\n",
       " 'and2',\n",
       " 'ane',\n",
       " 'ankrum',\n",
       " 'answeres',\n",
       " 'antrim',\n",
       " 'apertures',\n",
       " 'apple15',\n",
       " 'aprons',\n",
       " 'architectandsoothing',\n",
       " 'ariginal',\n",
       " 'array',\n",
       " 'asan',\n",
       " 'aspergers',\n",
       " 'asterix',\n",
       " 'athough',\n",
       " 'attention',\n",
       " 'auktyon',\n",
       " 'automoton',\n",
       " 'avril',\n",
       " 'azkaban',\n",
       " 'bachmann',\n",
       " 'baduk',\n",
       " 'balked',\n",
       " 'banished',\n",
       " 'barkley',\n",
       " 'basiclly',\n",
       " 'battlebegin',\n",
       " 'bealer',\n",
       " 'becaurfull',\n",
       " 'beforehand',\n",
       " 'belgian',\n",
       " 'beneficiary',\n",
       " 'berries',\n",
       " 'beutiful',\n",
       " 'bifocal',\n",
       " 'biogeography',\n",
       " 'biters',\n",
       " 'bladeplay',\n",
       " 'blightfully',\n",
       " 'blowhards',\n",
       " 'boaz',\n",
       " 'boll',\n",
       " 'book3',\n",
       " 'boots',\n",
       " 'botkin',\n",
       " 'boycot',\n",
       " 'brannon',\n",
       " 'breeds',\n",
       " 'brimley',\n",
       " 'brolan',\n",
       " 'brutaly',\n",
       " 'buehrens',\n",
       " 'bulova',\n",
       " 'burnette8',\n",
       " 'butterflies',\n",
       " 'bzkt',\n",
       " 'cafeterianot',\n",
       " 'callisto',\n",
       " 'camr',\n",
       " 'cansiones',\n",
       " 'captivating',\n",
       " 'carhartt',\n",
       " 'cars',\n",
       " 'cassis',\n",
       " 'catologe',\n",
       " 'ccomponents',\n",
       " 'centered',\n",
       " 'chace',\n",
       " 'channey',\n",
       " 'charicters',\n",
       " 'cheapskate',\n",
       " 'cheseborough',\n",
       " 'chime',\n",
       " 'chon',\n",
       " 'chronilogically',\n",
       " 'cinematograhy',\n",
       " 'ckish',\n",
       " 'clavat',\n",
       " 'clifftop',\n",
       " 'cloying',\n",
       " 'cobs',\n",
       " 'coincendtially',\n",
       " 'collie',\n",
       " 'combinbe',\n",
       " 'commending',\n",
       " 'compareable',\n",
       " 'complexion',\n",
       " 'comseeya',\n",
       " 'condering',\n",
       " 'confusing',\n",
       " 'conran',\n",
       " 'constraint',\n",
       " 'contininuing',\n",
       " 'conver',\n",
       " 'coordinated',\n",
       " 'coriandrum',\n",
       " 'corsair',\n",
       " 'councils',\n",
       " 'coverall',\n",
       " 'cramed',\n",
       " 'creats',\n",
       " 'cripness',\n",
       " 'crosstab',\n",
       " 'cryptx',\n",
       " 'culloden',\n",
       " 'curtural',\n",
       " 'cyd',\n",
       " 'dagon',\n",
       " 'dangeraus',\n",
       " 'dartboard',\n",
       " 'daytrips',\n",
       " 'deathball',\n",
       " 'decieds',\n",
       " 'deerso',\n",
       " 'defintite',\n",
       " 'delco',\n",
       " 'demanded',\n",
       " 'denota',\n",
       " 'derek',\n",
       " 'desertland',\n",
       " 'desvia',\n",
       " 'devereux',\n",
       " 'diagraming',\n",
       " 'didges',\n",
       " 'digipack',\n",
       " 'dink',\n",
       " 'disaposable',\n",
       " 'discotheque',\n",
       " 'disillusionment',\n",
       " 'displeasing',\n",
       " 'distace',\n",
       " 'divinyls',\n",
       " 'documentery',\n",
       " 'dominao',\n",
       " 'dopes',\n",
       " 'downable',\n",
       " 'draiman',\n",
       " 'dress',\n",
       " 'druglord',\n",
       " 'dud',\n",
       " 'dupa',\n",
       " 'dvdstretch',\n",
       " 'e5',\n",
       " 'easing',\n",
       " 'economical',\n",
       " 'eductor',\n",
       " 'egyptand',\n",
       " 'elder',\n",
       " 'elie',\n",
       " 'emaluate',\n",
       " 'emma',\n",
       " 'enamoured',\n",
       " 'endup',\n",
       " 'enjoyably',\n",
       " 'enterprises',\n",
       " 'envisaged',\n",
       " 'epsteinsmutha',\n",
       " 'eroding',\n",
       " 'esmer',\n",
       " 'estonians',\n",
       " 'europeans',\n",
       " 'everyman',\n",
       " 'exagerration',\n",
       " 'exclusions',\n",
       " 'exhortation',\n",
       " 'experiance',\n",
       " 'expounding',\n",
       " 'extrememly',\n",
       " 'fac',\n",
       " 'faithfull',\n",
       " 'fanfic',\n",
       " 'fasciitis',\n",
       " 'favirot',\n",
       " 'feeb',\n",
       " 'ferenc',\n",
       " 'fibonacci',\n",
       " 'fillings',\n",
       " 'finisher',\n",
       " 'fisherprice',\n",
       " 'flanders',\n",
       " 'fletching',\n",
       " 'florist',\n",
       " 'focals',\n",
       " 'fooeld',\n",
       " 'foreverxxx',\n",
       " 'fortuitous',\n",
       " 'fragmentation',\n",
       " 'fredo',\n",
       " 'fridays',\n",
       " 'frontpage',\n",
       " 'fukien',\n",
       " 'funtion',\n",
       " 'g54hp',\n",
       " 'gallop',\n",
       " 'garbeage',\n",
       " 'gatsby',\n",
       " 'gelatin',\n",
       " 'gentlman',\n",
       " 'getabstract',\n",
       " 'gigablock',\n",
       " 'gita',\n",
       " 'glimps',\n",
       " 'gnaw',\n",
       " 'goldbergs',\n",
       " 'goofiness',\n",
       " 'gouges',\n",
       " 'graininess',\n",
       " 'gratefull',\n",
       " 'greenface',\n",
       " 'gring',\n",
       " 'grouped',\n",
       " 'guardsmen',\n",
       " 'gummis',\n",
       " 'gwwild',\n",
       " 'hadley',\n",
       " 'hallima',\n",
       " 'handidly',\n",
       " 'happerned',\n",
       " 'harmonious',\n",
       " 'hastles',\n",
       " 'haworth',\n",
       " 'headspace',\n",
       " 'heavyweight',\n",
       " 'helio',\n",
       " 'hepinstall',\n",
       " 'hesitation',\n",
       " 'high12',\n",
       " 'hindering',\n",
       " 'hitman',\n",
       " 'hoisted',\n",
       " 'homenaje',\n",
       " 'hooha',\n",
       " 'horrific',\n",
       " 'housebroken',\n",
       " 'htem',\n",
       " 'humiliated',\n",
       " 'husted',\n",
       " 'hypocracy',\n",
       " 'icnd',\n",
       " 'ifthe',\n",
       " 'illusionary',\n",
       " 'immeditaley',\n",
       " 'implict',\n",
       " 'inaccurate3',\n",
       " 'incomparable',\n",
       " 'indentation',\n",
       " 'inedito',\n",
       " 'inflicted',\n",
       " 'inhis',\n",
       " 'inordinate',\n",
       " 'inspireing',\n",
       " 'insultingly',\n",
       " 'interdisciplinary',\n",
       " 'interpretted',\n",
       " 'intreging',\n",
       " 'investingate',\n",
       " 'irishness',\n",
       " 'ishalf',\n",
       " 'italy',\n",
       " 'iw',\n",
       " 'jambalaya9',\n",
       " 'javaer',\n",
       " 'jenell',\n",
       " 'jibed',\n",
       " 'johanssen',\n",
       " 'journeyed',\n",
       " 'juillet',\n",
       " 'jute',\n",
       " 'kamelot',\n",
       " 'katamari',\n",
       " 'keeps',\n",
       " 'kersh',\n",
       " 'kidde',\n",
       " 'kindnesses',\n",
       " 'kiyosaki',\n",
       " 'knopfler',\n",
       " 'konularn',\n",
       " 'kray',\n",
       " 'kuku',\n",
       " 'laboratories',\n",
       " 'lakeside',\n",
       " 'lankford',\n",
       " 'latarski',\n",
       " 'lawandorder',\n",
       " 'learned',\n",
       " 'legendsof',\n",
       " 'lensso',\n",
       " 'leverls',\n",
       " 'licked',\n",
       " 'lightweightpocket',\n",
       " 'lindas',\n",
       " 'lisening',\n",
       " 'livein',\n",
       " 'lobbying',\n",
       " 'loma2',\n",
       " 'loosers',\n",
       " 'lovedit',\n",
       " 'lucado',\n",
       " 'lunaire',\n",
       " 'lyndon',\n",
       " 'maccleary',\n",
       " 'madeas',\n",
       " 'magnitude',\n",
       " 'makescompetition',\n",
       " 'mamoulian',\n",
       " 'mankel',\n",
       " 'marathoner',\n",
       " 'marker',\n",
       " 'marty',\n",
       " 'masterson',\n",
       " 'matters',\n",
       " 'maysong',\n",
       " 'mcgillis',\n",
       " 'meagan',\n",
       " 'medio',\n",
       " 'melds',\n",
       " 'mendleev',\n",
       " 'merrells',\n",
       " 'method6',\n",
       " 'micheal',\n",
       " 'midwesterner',\n",
       " 'millitary',\n",
       " 'minisodes',\n",
       " 'misandrist',\n",
       " 'misrepresening',\n",
       " 'mixersregards',\n",
       " 'modeaf',\n",
       " 'molds',\n",
       " 'moniz',\n",
       " 'moonflower',\n",
       " 'morniong',\n",
       " 'motive',\n",
       " 'moviesor',\n",
       " 'muchachos',\n",
       " 'multifuntional',\n",
       " 'muscic',\n",
       " 'mutters',\n",
       " 'mysteris',\n",
       " 'najeeb',\n",
       " 'nasality',\n",
       " 'nazel',\n",
       " 'needyear',\n",
       " 'nephillimi',\n",
       " 'nevermore',\n",
       " 'niam',\n",
       " 'nights',\n",
       " 'nixed',\n",
       " 'non',\n",
       " 'north',\n",
       " 'notreally',\n",
       " 'nubian',\n",
       " 'nutri',\n",
       " 'obeserved',\n",
       " 'obtiene',\n",
       " 'odetsstarts',\n",
       " 'ofgrow',\n",
       " 'olajuwon',\n",
       " 'onc',\n",
       " 'oone',\n",
       " 'opposes',\n",
       " 'ordination',\n",
       " 'orlagh',\n",
       " 'ostreicher',\n",
       " 'outdid',\n",
       " 'outting',\n",
       " 'overintellectualizing',\n",
       " 'overused',\n",
       " 'paart',\n",
       " 'painhope',\n",
       " 'panadol',\n",
       " 'papyri',\n",
       " 'parissa',\n",
       " 'pasage',\n",
       " 'pathworking',\n",
       " 'pazzia',\n",
       " 'peddle',\n",
       " 'pencils',\n",
       " 'pequenas',\n",
       " 'peril',\n",
       " 'persico',\n",
       " 'peters',\n",
       " 'phasesas',\n",
       " 'photographing',\n",
       " 'pictoral',\n",
       " 'pimps',\n",
       " 'pitcairn',\n",
       " 'planall',\n",
       " 'playng',\n",
       " 'plotscarrie',\n",
       " 'podhoretz',\n",
       " 'politically2',\n",
       " 'pool',\n",
       " 'pors',\n",
       " 'possitive',\n",
       " 'pov',\n",
       " 'pragmatism',\n",
       " 'predicted',\n",
       " 'prepairing',\n",
       " 'presupositions',\n",
       " 'primero',\n",
       " 'probed',\n",
       " 'produto',\n",
       " 'prolog',\n",
       " 'proportioned',\n",
       " 'protuding',\n",
       " 'psicadelic',\n",
       " 'pubs',\n",
       " 'punkscene',\n",
       " 'pursuits',\n",
       " 'qc2',\n",
       " 'quatermain',\n",
       " 'quilts',\n",
       " 'rachal',\n",
       " 'rai',\n",
       " 'ramona',\n",
       " 'rascalssaid',\n",
       " 'rawr',\n",
       " 'readthatagain',\n",
       " 'rears',\n",
       " 'received2',\n",
       " 'recomemended',\n",
       " 'recruiting',\n",
       " 'reeboks',\n",
       " 'refreshed',\n",
       " 'regulari',\n",
       " 'rejuveniles',\n",
       " 'relistenin',\n",
       " 'remorseful',\n",
       " 'reparied',\n",
       " 'reppen',\n",
       " 'research',\n",
       " 'respiratory',\n",
       " 'retaliating',\n",
       " 'reveiw',\n",
       " 'rewires',\n",
       " 'richest',\n",
       " 'righwing',\n",
       " 'riuch',\n",
       " 'robotech',\n",
       " 'rol',\n",
       " 'roommate',\n",
       " 'rougeans',\n",
       " 'rts',\n",
       " 'rumsfeld',\n",
       " 'ryo',\n",
       " 'sacrefice',\n",
       " 'salaciousness',\n",
       " 'samoa',\n",
       " 'santour',\n",
       " 'satisfaite',\n",
       " 'sayian',\n",
       " 'scattered',\n",
       " 'schnaas',\n",
       " 'scond',\n",
       " 'screenplays',\n",
       " 'sd630',\n",
       " 'secessionist',\n",
       " 'seething',\n",
       " 'sementery',\n",
       " 'sentimentally',\n",
       " 'serivice',\n",
       " 'severeal',\n",
       " 'shahid',\n",
       " 'sharkvac',\n",
       " 'shelter',\n",
       " 'shinzon',\n",
       " 'shopping',\n",
       " 'shreader',\n",
       " 'sick',\n",
       " 'signings',\n",
       " 'simpho',\n",
       " 'sintow',\n",
       " 'skanned',\n",
       " 'skov',\n",
       " 'sledgehammer',\n",
       " 'slooooooowww',\n",
       " 'smarty',\n",
       " 'snakelike',\n",
       " 'snowcone',\n",
       " 'soething',\n",
       " 'soloists',\n",
       " 'song16',\n",
       " 'sopranino',\n",
       " 'soundscape',\n",
       " 'spaniard',\n",
       " 'spectacles',\n",
       " 'spielberg',\n",
       " 'splurged',\n",
       " 'springstein',\n",
       " 'squirming',\n",
       " 'staedtler',\n",
       " 'stant',\n",
       " 'starwarsuniverse',\n",
       " 'steesand',\n",
       " 'sthe',\n",
       " 'stocker',\n",
       " 'storyit',\n",
       " 'straws',\n",
       " 'strogatz',\n",
       " 'stunned',\n",
       " 'subliterate',\n",
       " 'succesfully',\n",
       " 'sugarland',\n",
       " 'suncka',\n",
       " 'superjumps',\n",
       " 'supprise',\n",
       " 'survivorship',\n",
       " 'swampy',\n",
       " 'swipes',\n",
       " 'symphonic',\n",
       " 't24',\n",
       " 'taijutsu',\n",
       " 'tamborine',\n",
       " 'targer',\n",
       " 'taymor',\n",
       " 'technicals',\n",
       " 'telefona',\n",
       " 'tenderizing',\n",
       " 'terrell',\n",
       " 'texturizing',\n",
       " 'thearpy',\n",
       " 'themind',\n",
       " 'thermopoliswhat',\n",
       " 'thieves',\n",
       " 'thoiseach',\n",
       " 'thrillerthat',\n",
       " 'thyroid',\n",
       " 'tilton',\n",
       " 'tiping',\n",
       " 'toadboy',\n",
       " 'tolast',\n",
       " 'tool',\n",
       " 'torre',\n",
       " 'tourÃ©',\n",
       " 'trades',\n",
       " 'transcriber',\n",
       " 'tratado',\n",
       " 'trenton',\n",
       " 'trinitarian',\n",
       " 'troup',\n",
       " 'tsettos',\n",
       " 'tung',\n",
       " 'tuymans',\n",
       " 'tycoons',\n",
       " 'uffda',\n",
       " 'umg',\n",
       " 'unbreakability',\n",
       " 'under13',\n",
       " 'underwire',\n",
       " 'unfeeling',\n",
       " 'unimpeachable',\n",
       " 'unload',\n",
       " 'unprinted',\n",
       " 'unsexy',\n",
       " 'unviewed',\n",
       " 'upscons',\n",
       " 'useless',\n",
       " 'uzis',\n",
       " 'valled',\n",
       " 'various',\n",
       " 'vehiclesi',\n",
       " 'verdaderamente',\n",
       " 'veterens',\n",
       " 'viderunt',\n",
       " 'vinyl',\n",
       " 'vitage',\n",
       " 'voir',\n",
       " 'vrey',\n",
       " 'wag',\n",
       " 'walnuts',\n",
       " 'waror',\n",
       " 'waterboarding',\n",
       " 'we',\n",
       " 'weeks',\n",
       " 'wendall',\n",
       " 'whacky',\n",
       " 'whimsey',\n",
       " 'whoever',\n",
       " 'wiggle',\n",
       " 'win8',\n",
       " 'wired',\n",
       " 'witout',\n",
       " 'womanlove',\n",
       " 'wordness',\n",
       " 'worthwile',\n",
       " 'wrinkled',\n",
       " 'wwv',\n",
       " 'xmidi',\n",
       " 'yanks',\n",
       " 'yesturday',\n",
       " 'yound',\n",
       " 'yvon',\n",
       " 'zeitoun',\n",
       " 'zisk',\n",
       " 'zzzzzzzzzzz']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Fit the CountVectorizer to the training data\n",
    "vect = CountVectorizer().fit(X_train)\n",
    "# Array mapping from feature integer indices to feature name\n",
    "vect.get_feature_names()[::200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have tokenized matrix of text document or reviews, you can use Logistic Regression or any other classifier to classify between the Negative and Positive Reviews for the limitation of this tutorial and just to show the intent of text classification and feature extraction techniques let us use logistic regression. Before you do that just have a look how feature matrix look like, using Vectorizer.transform() to make a document term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features: 126851\n"
     ]
    }
   ],
   "source": [
    "feature_name = vect.get_feature_names()\n",
    "print(\"Number of Features: {}\".format(len(feature_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of matrix (119892, 126851)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 113. GiB for an array with shape (119892, 126851) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-d4b34b5ab9af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX_train_vectorized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Shape of matrix'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_vectorized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_vectorized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1025\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1026\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1187\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1188\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1189\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 113. GiB for an array with shape (119892, 126851) and data type int64"
     ]
    }
   ],
   "source": [
    "# transform the documents in the training data to a document-term matrix\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "print('Shape of matrix', X_train_vectorized.shape)\n",
    "print(X_train_vectorized.toarray()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you are ready to build your first classification model, you are using sklearn.linear_model.LogisticRegression() from scikit learn as our first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression, SGDClassifier\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "model.fit(vect.transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Logistic Regression Output you can use AUC metric to validate or test your model on Test dataset, just to make sure how good a model is performing on new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8523519467454541\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "# Predict the transformed test documents\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, predictions)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8523004036966604\n",
      "precision: 0.8524629062074897\n",
      "recall: 0.8523004036966604\n",
      "F-measure: 0.8522933833876125\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "def eval_predictions(y_test, y_pred):\n",
    "    print ('accuracy:', metrics.accuracy_score(y_test, y_pred))\n",
    "    print ('precision:', metrics.precision_score(y_test, y_pred, average='weighted'))\n",
    "    print ('recall:', metrics.recall_score(y_test, y_pred, average='weighted'))\n",
    "    print ('F-measure:', metrics.f1_score(y_test, y_pred, average='weighted'))\n",
    "eval_predictions(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'False Positive Rate')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic(ROC)')\n",
    "plt.grid()\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.3f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn import svm\n",
    "# Create CV training and test scores for various training set sizes\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(LogisticRegression(),\n",
    "                                               vect.transform(X_train), y_train, cv=2, scoring='accuracy', n_jobs=-1,\n",
    "                                               # 50 different sizes of the training set\n",
    "                                               train_sizes=np.linspace(0.01, 1.0, 5))\n",
    "\n",
    "# Create means and standard deviations of training set scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create means and standard deviations of test set scores\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Draw lines\n",
    "plt.subplots(1, figsize=(7,7))\n",
    "plt.grid()\n",
    "plt.plot(train_sizes, train_mean, '--', color='green',  label=\"Training score\")\n",
    "plt.plot(train_sizes, test_mean, color='red', label=\"Test score\")\n",
    "\n",
    "    # Draw bands\n",
    "#plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n",
    "#plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n",
    "\n",
    "    # Create plot\n",
    "plt.title(\"Learning Curve for Logistic Regression\")\n",
    "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd MOdel - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                       class_weight={0: 10, 1: 12}, criterion='entropy',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       max_samples=None, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       n_estimators=2, n_jobs=None, oob_score=False,\n",
       "                       random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.linear_model import RandomForest,SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Train the model\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=2, class_weight={0:10, 1:12}, \\\n",
    "                                criterion=\"entropy\", random_state=42)\n",
    "forest.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.6275784838481578\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "# Predict the transformed test documents\n",
    "predictions = forest.predict(vect.transform(X_test))\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, predictions)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.6287325259400127\n",
      "precision: 0.654662315246596\n",
      "recall: 0.6287325259400127\n",
      "F-measure: 0.6115282793990658\n"
     ]
    }
   ],
   "source": [
    "eval_predictions(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'False Positive Rate')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic(ROC) - Random Forest')\n",
    "plt.grid()\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.3f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-14cde630b98f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Create CV training and test scores for various training set sizes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m train_sizes, train_scores, test_scores = learning_curve(RandomForestRegressor(),\n\u001b[1;32m----> 6\u001b[1;33m                                                vect.transform(X_train), y_train, cv=3)\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Create means and standard deviations of training set scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mlearning_curve\u001b[1;34m(estimator, X, y, groups, train_sizes, cv, scoring, exploit_incremental_learning, n_jobs, pre_dispatch, verbose, shuffle, random_state, error_score, return_times)\u001b[0m\n\u001b[0;32m   1254\u001b[0m             \u001b[0mparameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1255\u001b[0m             error_score=error_score, return_times=return_times)\n\u001b[1;32m-> 1256\u001b[1;33m             for train, test in train_test_proportions)\n\u001b[0m\u001b[0;32m   1257\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m         \u001b[0mn_cv_folds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mn_unique_ticks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    833\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    513\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    381\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m                     n_samples_bootstrap=n_samples_bootstrap)\n\u001b[1;32m--> 383\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    833\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1223\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1224\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1225\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1226\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    365\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create CV training and test scores for various training set sizes\n",
    "train_sizes, train_scores, test_scores = learning_curve(RandomForestRegressor(),\n",
    "                                               vect.transform(X_train), y_train, cv=3)\n",
    "\n",
    "# Create means and standard deviations of training set scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create means and standard deviations of test set scores\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Draw lines\n",
    "plt.subplots(1, figsize=(7,7))\n",
    "plt.grid()\n",
    "plt.plot(train_sizes, train_mean, '--', color='green',  label=\"Training score\")\n",
    "plt.plot(train_sizes, test_mean, color='red', label=\"Cross-validation score\")\n",
    "\n",
    "    # Draw bands\n",
    "#plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n",
    "#plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n",
    "\n",
    "    # Create plot\n",
    "plt.title(\"Learning Curve for Random Forest\")\n",
    "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3rd Model - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "# instantiate and train model, kernel=rbf \n",
    "svm_rbf = svm.SVC(random_state=12345)\n",
    "svm_rbf.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# evaulate model\n",
    "y_pred_1 = svm_rbf.predict(vect.transform(X_test))\n",
    "print(\" SVM : \", y_pred_1)\n",
    "\n",
    "\n",
    "\n",
    "eval_predictions(y_train, y_pred_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn import svm\n",
    "# Create CV training and test scores for various training set sizes\n",
    "train_sizes, train_scores, test_scores = learning_curve(svm.SVC(),\n",
    "                                               vect.transform(X_test), y_train, cv=3, scoring='accuracy', n_jobs=-1,\n",
    "                                               # 50 different sizes of the training set\n",
    "                                               train_sizes=np.linspace(0.01, 1.0, 50))\n",
    "\n",
    "# Create means and standard deviations of training set scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create means and standard deviations of test set scores\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Draw lines\n",
    "plt.subplots(1, figsize=(7,7))\n",
    "plt.grid()\n",
    "plt.plot(train_sizes, train_mean, '--', color='green',  label=\"Training score\")\n",
    "plt.plot(train_sizes, test_mean, color='red', label=\"Cross-validation score\")\n",
    "\n",
    "    # Draw bands\n",
    "#plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color=\"#DDDDDD\")\n",
    "#plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color=\"#DDDDDD\")\n",
    "\n",
    "    # Create plot\n",
    "plt.title(\"Learning Curve for SVC\")\n",
    "plt.xlabel(\"Training Set Size\"), plt.ylabel(\"Accuracy Score\"), plt.legend(loc=\"best\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "# Predict the transformed test documents\n",
    "predictions = svm_rbf.predict(vect.transform(X_test))\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, predictions)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic(ROC) - SVC')\n",
    "plt.grid()\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.3f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = vect.transform(X_test)\n",
    "y = y_train\n",
    "    \n",
    "clf1 = DecisionTreeClassifier(criterion='entropy', max_depth=1)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=1)    \n",
    "\n",
    "bagging1 = BaggingClassifier(base_estimator=clf1, n_estimators=10, max_samples=0.8, max_features=0.8)\n",
    "bagging2 = BaggingClassifier(base_estimator=clf2, n_estimators=10, max_samples=0.8, max_features=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['Decision Tree', 'K-NN', 'Bagging Tree', 'Bagging K-NN']\n",
    "clf_list = [clf1, clf2, bagging1, bagging2]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "for clf, label, grd in zip(clf_list, label, grid):        \n",
    "    scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
    "    print \"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label)\n",
    "        \n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
    "    plt.title(label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot learning curves\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "plt.figure()\n",
    "plot_learning_curves(X_train, y_train, X_test, y_test, bagging1, print_model=False, style='ggplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensemble Size\n",
    "num_est = map(int, np.linspace(1,100,20))\n",
    "bg_clf_cv_mean = []\n",
    "bg_clf_cv_std = []\n",
    "for n_est in num_est:    \n",
    "    bg_clf = BaggingClassifier(base_estimator=clf1, n_estimators=n_est, max_samples=0.8, max_features=0.8)\n",
    "    scores = cross_val_score(bg_clf, X, y, cv=3, scoring='accuracy')\n",
    "    bg_clf_cv_mean.append(scores.mean())\n",
    "    bg_clf_cv_std.append(scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "(_, caps, _) = plt.errorbar(num_est, bg_clf_cv_mean, yerr=bg_clf_cv_std, c='blue', fmt='-o', capsize=5)\n",
    "for cap in caps:\n",
    "    cap.set_markeredgewidth(1)                                                                                                                                \n",
    "plt.ylabel('Accuracy'); plt.xlabel('Ensemble Size'); plt.title('Bagging Tree Ensemble');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from mlxtend.plotting import plot_decision_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vect.transform(X_test)\n",
    "y = y_train\n",
    "    \n",
    "#XOR dataset\n",
    "#X = np.random.randn(200, 2)\n",
    "#y = np.array(map(int,np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)))\n",
    "    \n",
    "clf = DecisionTreeClassifier(criterion='entropy', max_depth=1)\n",
    "\n",
    "num_est = [1, 2, 3, 10]\n",
    "label = ['AdaBoost (n_est=1)', 'AdaBoost (n_est=2)', 'AdaBoost (n_est=3)', 'AdaBoost (n_est=10)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "for n_est, label, grd in zip(num_est, label, grid):     \n",
    "    boosting = AdaBoostClassifier(base_estimator=clf, n_estimators=n_est)   \n",
    "    boosting.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=boosting, legend=2)\n",
    "    plt.title(label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot learning curves\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "boosting = AdaBoostClassifier(base_estimator=clf, n_estimators=10)\n",
    "        \n",
    "plt.figure()\n",
    "plot_learning_curves(X_train, y_train, X_test, y_test, boosting, print_model=False, style='ggplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensemble Size\n",
    "num_est = map(int, np.linspace(1,100,20))\n",
    "bg_clf_cv_mean = []\n",
    "bg_clf_cv_std = []\n",
    "for n_est in num_est:\n",
    "    ada_clf = AdaBoostClassifier(base_estimator=clf, n_estimators=n_est)\n",
    "    scores = cross_val_score(ada_clf, X, y, cv=3, scoring='accuracy')\n",
    "    bg_clf_cv_mean.append(scores.mean())\n",
    "    bg_clf_cv_std.append(scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "(_, caps, _) = plt.errorbar(num_est, bg_clf_cv_mean, yerr=bg_clf_cv_std, c='blue', fmt='-o', capsize=5)\n",
    "for cap in caps:\n",
    "    cap.set_markeredgewidth(1)                                                                                                                                \n",
    "plt.ylabel('Accuracy'); plt.xlabel('Ensemble Size'); plt.title('AdaBoost Ensemble');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "from mlxtend.plotting import plot_learning_curves\n",
    "from mlxtend.plotting import plot_decision_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vect.transform(X_test)\n",
    "y = y_train\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "lr = LogisticRegression()\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], \n",
    "                          meta_classifier=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['KNN', 'Random Forest', 'Naive Bayes', 'Stacking Classifier']\n",
    "clf_list = [clf1, clf2, clf3, sclf]\n",
    "    \n",
    "fig = plt.figure(figsize=(10,8))\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "grid = itertools.product([0,1],repeat=2)\n",
    "\n",
    "clf_cv_mean = []\n",
    "clf_cv_std = []\n",
    "for clf, label, grd in zip(clf_list, label, grid):\n",
    "        \n",
    "    scores = cross_val_score(clf, X, y, cv=3, scoring='accuracy')\n",
    "    print \"Accuracy: %.2f (+/- %.2f) [%s]\" %(scores.mean(), scores.std(), label)\n",
    "    clf_cv_mean.append(scores.mean())\n",
    "    clf_cv_std.append(scores.std())\n",
    "        \n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf)\n",
    "    plt.title(label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot classifier accuracy    \n",
    "plt.figure()\n",
    "(_, caps, _) = plt.errorbar(range(4), clf_cv_mean, yerr=clf_cv_std, c='blue', fmt='-o', capsize=5)\n",
    "for cap in caps:\n",
    "    cap.set_markeredgewidth(1)                                                                                                                                \n",
    "plt.xticks(range(4), ['KNN', 'RF', 'NB', 'Stacking'])        \n",
    "plt.ylabel('Accuracy'); plt.xlabel('Classifier'); plt.title('Stacking Ensemble');\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot learning curves\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "plt.figure()\n",
    "plot_learning_curves(X_train, y_train, X_test, y_test, sclf, print_model=False, style='ggplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Wow, looks like a Pretty good Score! for initial modelling, let us try out if you can further improve your model score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-Idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already discussed earlier you will be using Tf-Idf technique, in this section you are going to create your document term matrix using TfidfVectorizer() available within sklearn.\n",
    "\n",
    "There are some parameters which needs to be defined while building vocabullary or Tf-Idf matrix such as, min_df and max_df.\n",
    "\n",
    "min_df is used for removing terms that appear too infrequently. For example:\n",
    "min_df = 0.01 means \"ignore terms that appear in less than 1% of the documents\".\n",
    "min_df = 5 means \"ignore terms that appear in less than 5 documents\".\n",
    "The default min_df is 1.0, which means \"ignore terms that appear in less than 1 document\". Thus, the default setting does not ignore any terms.\n",
    "\n",
    "max_df is used for removing terms that appear too frequently, also known as \"corpus-specific stop words\". For example:\n",
    "max_df = 0.50 means \"ignore terms that appear in more than 50% of the documents\".\n",
    "max_df = 25 means \"ignore terms that appear in more than 25 documents\".\n",
    "The default max_df is 1.0, which means \"ignore terms that appear in more than 100% of the documents\". Thus, the default setting does not ignore any terms.\n",
    "\n",
    "Setting min_df = 5 and max_df = 1.0 (default)\n",
    "Which means while building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold, which means not keeping words those do not occur in atleast 5 documents or reviews (in our context), this can be considered as a hyperparmater which directly affects accuracy of your model so you need to do a trial or a grid search to find what value of min_df or max_df gives best result, again it highly depends on your data. This value is also called cut-off in the literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5; \n",
    "# you can play with it and see how accuracy changes \n",
    "vect_tf = TfidfVectorizer(min_df=5000).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = vect_tf.get_feature_names()\n",
    "print(\"Number of Features: {}\".format(len(feature_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized_tf = vect.transform(X_train)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized_tf, y_train)\n",
    "predictions_tf = model.predict(vect.transform(X_test))\n",
    "print('AUC: ', roc_auc_score(y_test, predictions_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, predictions)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.grid()\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.3f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test, predictions_tf)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=[0,1]\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"coolwarm\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def eval_predictions(y_test, y_pred):\n",
    "    print ('accuracy:', metrics.accuracy_score(y_test, y_pred))\n",
    "    print ('precision:', metrics.precision_score(y_test, y_pred, average='weighted'))\n",
    "    print ('recall:', metrics.recall_score(y_test, y_pred, average='weighted'))\n",
    "    print ('F-measure:', metrics.f1_score(y_test, y_pred, average='weighted'))\n",
    "eval_predictions(y_test, predictions_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.3f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict_proba(X_train_vectorized)[::,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(y_train,  y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = metrics.roc_auc_score(y_train, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.grid()\n",
    "plt.title(\"ROC\")\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of count, TF and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "\n",
    "def build_model(mode):\n",
    "    # Intent to use default paramaters for show case\n",
    "    vect = None\n",
    "    if mode == 'count':\n",
    "        vect = CountVectorizer()\n",
    "    elif mode == 'tf':\n",
    "        vect = TfidfVectorizer(use_idf=False, norm='l2')\n",
    "    elif mode == 'tfidf':\n",
    "        vect = TfidfVectorizer()\n",
    "    else:\n",
    "        raise ValueError('Mode should be either count or tfidf')\n",
    "    \n",
    "    return Pipeline([\n",
    "        ('vect', vect),\n",
    "        ('clf' , LogisticRegression(solver='newton-cg',n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "def pipeline(X, y, mode):\n",
    "    processed_x = x\n",
    "    \n",
    "    model_pipeline = build_model(mode)\n",
    "    cv = KFold(n_splits=5, shuffle=True)\n",
    "    \n",
    "    scores = cross_val_score(model_pipeline, processed_x, y, cv=cv, scoring='accuracy')\n",
    "    print(\"Accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))\n",
    "    \n",
    "    return model_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_train\n",
    "y = y_train\n",
    "    \n",
    "model_pipeline = build_model(mode='count')\n",
    "model_pipeline.fit(x, y)\n",
    "\n",
    "print('Number of Vocabulary: %d'% (len(model_pipeline.named_steps['vect'].get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Using Count Vectorizer------')\n",
    "model_pipeline = pipeline(X_train, y_train, mode='count')\n",
    "\n",
    "print('Using TF Vectorizer------')\n",
    "model_pipeline = pipeline(X_train, y_train, mode='tf')\n",
    "\n",
    "print('Using TF-IDF Vectorizer------')\n",
    "model_pipeline = pipeline(X_train, y_train, mode='tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " introduce the usage of n-grams terms in our model and see the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the CountVectorizer to the training data specifiying a minimum \n",
    "# document frequency of 5 and extracting 1-grams and 2-grams\n",
    "vect = CountVectorizer(min_df=5, ngram_range=(2,3)).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, predictions)\n",
    "#roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_test, predictions)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names=[0,1]\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"coolwarm\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can usng n-grams technique which increases the accuracy of model, but why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.3f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.grid()\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypertuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def count_vec_ngram(params, X_train, y_train):\n",
    "    lr = LogisticRegression(random_state=1)\n",
    "    cvec_p = CountVectorizer(ngram_range=(params))\n",
    "    cvec_p.fit(X_train)\n",
    "    X_train_cvec_p = cvec_p.transform(X_train)\n",
    "    # cross val score/ predict\n",
    "    cvec_score_p = cross_val_score(lr, X_train_cvec_p, y_train, cv=3)\n",
    "    # cross validation \n",
    "    return cvec_score_p.mean()\n",
    "\n",
    "params = [(1,1), (1,2),(1,3), (1,4)] \n",
    "ngram_scores = []\n",
    "for p in params:\n",
    "    ngram_scores.append(count_vec_ngram(p, X_train, y_train))\n",
    "plt.title('Accuracy of ngram Range')\n",
    "ngrams = ['gram_1','gram_2','gram_3','gram_4']\n",
    "ngram_df = pd.DataFrame({'params':ngrams, 'scores':ngram_scores}, index=[0,1,2,3])\n",
    "# adding cvec score with default params\n",
    "ngram_df = ngram_df.append(df.iloc[:1,:])\n",
    "# plot scores on graph\n",
    "sns.pointplot(x='params', y='scores', data =ngram_df)\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.grid()\n",
    "plt.xlabel('ngrams')\n",
    "plt.xticks(rotation=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def count_vec_ngram(params, X_train, y_train):\n",
    "    lr = LogisticRegression(random_state=1)\n",
    "    cvec_p = TfidfVectorizer(ngram_range=(params))\n",
    "    cvec_p.fit(X_train)\n",
    "    X_train_cvec_p = cvec_p.transform(X_train)\n",
    "    # cross val score/ predict\n",
    "    cvec_score_p = cross_val_score(lr, X_train_cvec_p, y_train, cv=3)\n",
    "    # cross validation \n",
    "    return cvec_score_p.mean()\n",
    "\n",
    "params = [(1,1), (1,2),(1,3), (1,4)] \n",
    "ngram_scores = []\n",
    "for p in params:\n",
    "    ngram_scores.append(count_vec_ngram(p, X_train, y_train))\n",
    "    \n",
    "ngrams = ['gram_1','gram_2','gram_3','gram_4']\n",
    "ngram_df = pd.DataFrame({'params':ngrams, 'scores':ngram_scores}, index=[0,1,2,3])\n",
    "# adding cvec score with default params\n",
    "ngram_df = ngram_df.append(df.iloc[:1,:])\n",
    "# plot scores on graph\n",
    "sns.pointplot(x='params', y='scores', data =ngram_df)\n",
    "plt.title('Accuracy of ngram Range')\n",
    "plt.grid()\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.xlabel('ngrams')\n",
    "plt.xticks(rotation=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=1)\n",
    "def count_vec_max_features(params, X_train, y_train):\n",
    "    cvec_p = CountVectorizer(max_features=params)\n",
    "    cvec_p.fit(X_train)\n",
    "    X_train_cvec_p = cvec_p.transform(X_train)\n",
    "    # cross val score/ predict\n",
    "    cvec_score_p = cross_val_score(lr, X_train_cvec_p, y_train, cv=3)\n",
    "    # cross validation \n",
    "    return cvec_score_p.mean()\n",
    "\n",
    "mf_params = [None, 500, 1000, 5000, 10000]\n",
    "max_features_scores = [count_vec_max_features(p, X_train, y_train) for p in mf_params]\n",
    "max_features = ['max_f_'+str(p) for p in mf_params]\n",
    "# dataframe for scores\n",
    "max_features_df = pd.DataFrame({'params':max_features, 'scores':max_features_scores}, index=[0,1,2,3,4])\n",
    "# adding cvec score with default params\n",
    "max_features_df = max_features_df.append(df.iloc[:1,:])\n",
    "sns.pointplot(x='params', y='scores', data =max_features_df)\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.xlabel('Max Features')\n",
    "plt.grid()\n",
    "plt.xticks(rotation=40)\n",
    "plt.title('Accuracy of Max Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=1)\n",
    "def count_vec_max_features(params, X_train, y_train):\n",
    "    cvec_p = TfidfVectorizer(max_features=params)\n",
    "    cvec_p.fit(X_train)\n",
    "    X_train_cvec_p = cvec_p.transform(X_train)\n",
    "    # cross val score/ predict\n",
    "    cvec_score_p = cross_val_score(lr, X_train_cvec_p, y_train, cv=3)\n",
    "    # cross validation \n",
    "    return cvec_score_p.mean()\n",
    "\n",
    "mf_params = [None, 500, 1000, 5000, 10000]\n",
    "max_features_scores = [count_vec_max_features(p, X_train, y_train) for p in mf_params]\n",
    "max_features = ['max_f_'+str(p) for p in mf_params]\n",
    "# dataframe for scores\n",
    "max_features_df = pd.DataFrame({'params':max_features, 'scores':max_features_scores}, index=[0,1,2,3,4])\n",
    "# adding cvec score with default params\n",
    "max_features_df = max_features_df.append(df.iloc[:1,:])\n",
    "sns.pointplot(x='params', y='scores', data =max_features_df)\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.xlabel('Max Features')\n",
    "plt.grid()\n",
    "plt.xticks(rotation=40)\n",
    "plt.title('Accuracy of Max Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAx df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore terms that appear in more than 50% of the documents\n",
    "max_df = 0.50\n",
    "# ignore terms that appear in more than 25 documents\n",
    "max_df = 25\n",
    "# DEFAULT\n",
    "# ignore terms that appear in more than 100% of the documents \n",
    "# doesnt ignore anything \n",
    "max_df = 1.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vec_max_df(params, X_train, y_train):\n",
    "    cvec_p = CountVectorizer(max_df=params)\n",
    "    cvec_p.fit(X_train)\n",
    "    X_train_cvec_p = cvec_p.transform(X_train)\n",
    "    # cross val score/ predict\n",
    "    cvec_score_p = cross_val_score(lr, X_train_cvec_p, y_train, cv=3)\n",
    "    # cross validation \n",
    "    return cvec_score_p.mean()\n",
    "\n",
    "mdf_params = [0.25, 0.5, 0.75, 1.0]\n",
    "max_df_scores = [count_vec_max_df(p, X_train, y_train) for p in mdf_params]\n",
    "max_df = ['max_df_'+str(p) for p in mdf_params]\n",
    "# dataframe for scores\n",
    "max_df_df = pd.DataFrame({'params':max_df, 'scores':max_df_scores}, index=[0,1,2,3])\n",
    "# adding cvec score with default params\n",
    "max_df_df = max_df_df.append(df.iloc[:1,:])\n",
    "sns.pointplot(x='params', y='scores', data =max_df_df)\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.xlabel('max_df')\n",
    "plt.xticks(rotation=40)\n",
    "plt.title('Accuracy of Max df')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vec_max_df(params, X_train, y_train):\n",
    "    cvec_p = TfidfVectorizer(max_df=params)\n",
    "    cvec_p.fit(X_train)\n",
    "    X_train_cvec_p = cvec_p.transform(X_train)\n",
    "    # cross val score/ predict\n",
    "    cvec_score_p = cross_val_score(lr, X_train_cvec_p, y_train, cv=3)\n",
    "    # cross validation \n",
    "    return cvec_score_p.mean()\n",
    "\n",
    "mdf_params = [0.25, 0.5, 0.75, 1.0]\n",
    "max_df_scores = [count_vec_max_df(p, X_train, y_train) for p in mdf_params]\n",
    "max_df = ['max_df_'+str(p) for p in mdf_params]\n",
    "# dataframe for scores\n",
    "max_df_df = pd.DataFrame({'params':max_df, 'scores':max_df_scores}, index=[0,1,2,3])\n",
    "# adding cvec score with default params\n",
    "max_df_df = max_df_df.append(df.iloc[:1,:])\n",
    "sns.pointplot(x='params', y='scores', data =max_df_df)\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.xlabel('max_df')\n",
    "plt.grid()\n",
    "plt.xticks(rotation=40)\n",
    "plt.title('Accuracy of Max df')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update accuracy scores with highest score for 1,2 ngram\n",
    "acc_df1 = df.append(ngram_df.iloc[3,:])\n",
    "acc_df1.reset_index(inplace=True, drop=True)\n",
    "acc_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update accuracy dataframe with 3 highest scores\n",
    "acc_df2 = acc_df1.append(max_features_df.drop(max_features_df.index[[1,2]]))\n",
    "acc_df2.reset_index(inplace=True, drop=True)\n",
    "acc_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update accuracy dataframe\n",
    "acc_df3 = acc_df2.append(max_df_df.iloc[:2,:])\n",
    "acc_df3.reset_index(inplace=True, drop=True)\n",
    "acc_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(x='params', y='scores', data =acc_df3)\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.xlabel('Parameters')\n",
    "plt.xticks(rotation=40)\n",
    "plt.title('Comparison of Highest Parameters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# initalise the vectoriser \n",
    "cvec_b = CountVectorizer(ngram_range=(1,4), max_df=0.25)\n",
    "# fit the training data on the model\n",
    "cvec_b.fit(X_train)\n",
    "\n",
    "#transform training data into sparse matrix\n",
    "X_train_cvec = cvec_b.transform(X_train)\n",
    "\n",
    "# cross val score/ predict\n",
    "cvec_score = cross_val_score(lr, X_train_cvec, y_train, cv=3)\n",
    "cvec_score.mean()\n",
    "\n",
    "\n",
    "acc_df3.loc[2]= ['best_params', cvec_score.mean()]\n",
    "acc_df3.sort_values('scores', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pointplot(x='params', y='scores', data =acc_df3)\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.xlabel('Parameters')\n",
    "plt.xticks(rotation=40)\n",
    "plt.title('Comparison of Highest Parameters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform features once!\n",
    "cvec_p = CountVectorizer(ngram_range=(1,4)) \n",
    "cvec_p.fit(X_train)\n",
    "X_train_cvec_p = cvec_p.transform(X_train)\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "# fit with l1 \n",
    "model_l1 = LogisticRegressionCV(Cs=np.logspace(-10,10,21),penalty = 'l1',solver='liblinear',cv=3) \n",
    "model_l1.fit(X_train_cvec_p, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit with l2\n",
    "model_l2 = LogisticRegressionCV(Cs=np.logspace(-10,10,21), penalty = 'l2',solver='liblinear',cv=3) \n",
    "model_l2.fit(X_train_cvec_p, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the different penalties of each class\n",
    "for i, cl in enumerate(model_l1.classes_):\n",
    " plt.plot(model_l1.Cs_,model_l1.scores_.values()[i].mean(axis=0),label=â€™l1')\n",
    " plt.plot(model_l2.Cs_,model_l2.scores_.values()[i].mean(axis=0),alpha=0.7,label=â€™l2')\n",
    " plt.xscale(â€˜logâ€™)\n",
    " plt.xlabel(â€˜Câ€™)\n",
    " plt.ylabel(â€˜accuracyâ€™)\n",
    " plt.legend(loc=â€™lower rightâ€™)\n",
    " plt.title(â€˜Class {}â€™.format(cl))\n",
    " plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.score(X_train_vectorized, y_train) # accuracy\n",
    "print(\"Logistic Regression : \" , scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaulate model\n",
    "y_pred_1 = model.predict(X_train_vectorized)\n",
    "print(\" LR  : \", y_pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def eval_predictions(y_test, y_pred):\n",
    "    print ('accuracy:', metrics.accuracy_score(y_test, y_pred_1))\n",
    "    print ('precision:', metrics.precision_score(y_test, y_pred, average='weighted'))\n",
    "    print ('recall:', metrics.recall_score(y_test, y_pred, average='weighted'))\n",
    "    print ('F-measure:', metrics.f1_score(y_test, y_pred, average='weighted'))\n",
    "eval_predictions(y_train, y_pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=2, class_weight={1:10,  0:1}, \\\n",
    "                                criterion=\"entropy\", random_state=42)\n",
    "forest.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = forest.predict(X_train_vectorized)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_train, y_pred)\n",
    "f1_score = metrics.f1_score(y_train, y_pred, average=\"micro\")\n",
    "\n",
    "print(f\"Random Forest Accuracy: {accuracy*100:.3f}%\")\n",
    "print(f\"Random Forest Accuracy: {f1_score*100:.3f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train_vectorized,\n",
    "                                                   mode = 'regression',\n",
    "                                                   feature_names = feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exp = explainer.explain_instance(X_train_vectorized, rf.predict, num_features=5)\n",
    "exp.show_in_notebook(show_all=False) #only the features used in the explanation are displayed\n",
    "\n",
    "\n",
    "exp = explainer.explain_instance(X_train_vectorized, rf.predict, num_features=5)\n",
    "exp.show_in_notebook(show_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr):\n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = forest.predict_proba(X_train_vectorized)[::,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_train, probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(y_train, probs)\n",
    "print('AUC: %.2f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(class_weight={1:10, 0:12}, \\\n",
    "                                random_state=42)\n",
    "#model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "predictions = model.predict(X_train_vectorized)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_train, predictions)\n",
    "f1_score = metrics.f1_score(y_train, predictions, average=\"micro\")\n",
    "\n",
    "print(f\"Training Set Accuracy: {accuracy*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(y_train, predictions)\n",
    "f1_score = metrics.f1_score(y_train, predictions, average=\"micro\")\n",
    "\n",
    "print(f\"Training Set Accuracy: {accuracy*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "f, axes = plt.subplots(figsize=(20,7))\n",
    "ax = sns.countplot(x=df[\"Positively_Rated\"], palette=\"OrRd_r\")\n",
    "ax.set(title=\"Distribution of Product Ratings\", \\\n",
    "       xlabel=\"Positively_Rated\", ylabel=\"Number of Reviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name = vect.get_feature_names()\n",
    "print(\"Number of Features: {}\".format(len(feature_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_list = model_df[model_df[0].isna()].index\n",
    "nan_list = nan_list.tolist()\n",
    "\n",
    "print(nan_list[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*[tokenized[blank] for blank in nan_list[0:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for blank in nan_list[0:5]:\n",
    "    display(df[\"reviewText\"].iloc[blank])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original 'model_df' count: {len(model_df)}\")\n",
    "print(f\"Final 'model_df' count: {len(model_df.dropna(axis=0))}\")\n",
    "\n",
    "model_df = model_df.dropna(axis=0)\n",
    "display(model_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority = df[\"overall\"] ==5\n",
    "majority_ratio = len(df[majority]) / len(df)\n",
    "\n",
    "print(f\"{majority_ratio*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = len(model_df[model_df[\"label\"] == 2])\n",
    "print(f\"Size of the most underrepresented class: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trim the majority class\n",
    "count = len(model_df[model_df[\"label\"] == 2])\n",
    "condition = model_df[\"label\"] == 5\n",
    "trimmed_df = model_df[condition].sample(n=count, random_state=42, replace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#trim other class and add on to the trimmed_df\n",
    "for rating in [1, 2, 3, 4]:\n",
    "    condition = model_df[\"label\"] == rating\n",
    "    if len(model_df[condition]) >= count:\n",
    "        add_df = model_df[condition].sample(n=count, random_state=42)\n",
    "    else:\n",
    "        add_df = model_df[condition]\n",
    "    trimmed_df = pd.concat([trimmed_df, add_df], ignore_index=False)\n",
    "\n",
    "#display new class sizes of trimmed_df\n",
    "for rating in [1, 2, 3, 4, 5]:\n",
    "    class_size = len(trimmed_df[trimmed_df[\"label\"] == rating])\n",
    "    print(f\"Size of Class {rating}: {class_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = trimmed_df.iloc[:, :-1]\n",
    "y = trimmed_df.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "label_shape = np.shape(y_test)\n",
    "y_baseline = np.full(label_shape, 5)\n",
    "\n",
    "accuracy_baseline = metrics.accuracy_score(y_test, y_baseline)\n",
    "f1_score_baseline = metrics.f1_score(y_test, y_baseline, average=\"micro\")\n",
    "\n",
    "print(f\"Baseline Accuracy: {accuracy_baseline*100:.3f}%\")\n",
    "print(f\"Baseline F1 Score: {f1_score_baseline:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#sampling the model_df population\n",
    "pca_df = model_df.reset_index()\n",
    "pca_df = model_df.dropna(axis=0).iloc[:,1:]\n",
    "pca_df = pca_df.iloc[::50]\n",
    "\n",
    "#setting up PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca = pca.fit_transform(pca_df.iloc[:, :-1])\n",
    "labels = pca_df[\"label\"]\n",
    "\n",
    "#setting up plot components\n",
    "x_axis = pca[:,0]\n",
    "y_axis = pca[:,1]\n",
    "color_map = pca_df[\"label\"].map({1:\"blue\", \\\n",
    "                                 2:\"red\", \\\n",
    "                                 3:\"yellow\", \\\n",
    "                                 4:\"green\", \\\n",
    "                                 5:\"orange\"})\n",
    "\n",
    "#plotting PCA\n",
    "f, axes = plt.subplots(figsize=(20,10))\n",
    "plt.scatter(x_axis, y_axis, color=color_map, s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis #\n",
    "We'll implement several interesting Natural Language Processing techniques in order to explore our Amazon dataset.\n",
    "\n",
    "## More on Word2Vec ##\n",
    "To better appreciate the concept of word embeddings, we take five common words in our corpora and derive their five most related words using our `word_vec` model. The similarity comes from how often these tokens appear in the same window of words as their `word_bank` counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_bank = [\"nook\", \"phone\", \"tv\", \"good\", \"price\"]\n",
    "\n",
    "for word in word_bank[:]:\n",
    "    related_vec = word_vec.wv.most_similar(word, topn=5)\n",
    "    related_words = np.array(related_vec)[:,0]\n",
    "    word_bank.extend(related_words)\n",
    "    print(f\"{word}: {related_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE ###\n",
    "Like PCA, the t-Distributed Stochastic Neighbor Embedding (*t-SNE*) is another dimensionality reduction technique that assists in visualizing high-dimensional datasets. To perceive the similarity between the related words in terms of spatial distance, t-SNE provided the coordinates of each word in a 2D scatterplot plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=5, n_iter=1000, random_state=42)\n",
    "\n",
    "sample_vecs = word_vec.wv[set(word_bank)]\n",
    "sample_tsne = tsne.fit_transform(sample_vecs)\n",
    "tsne_x = sample_tsne[:, 0]\n",
    "tsne_y = sample_tsne[:, 1]\n",
    "\n",
    "f, axes = plt.subplots(figsize=(20,7))\n",
    "ax = plt.scatter(x=tsne_x, y=tsne_y)\n",
    "\n",
    "for label, x, y in zip(word_bank, tsne_x, tsne_y):\n",
    "    plt.annotate(label, xy=(x+3, y+3))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Algebra ###\n",
    "Since *Word2Vec* characterizes words into quantified tokens, we can consequently add or subtract word vectors together. To add is to combine the meaning of the components and to subtract is to take out the context of one token from another. The following are examples of this vector algebra and their similarity scores:\n",
    "\n",
    "**Books + Touchscreen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec.wv.most_similar(positive=[\"books\", \"touchscreen\"], \\\n",
    "                      negative=[], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cheap â€“ Quality**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec.wv.most_similar(positive=[\"cheap\"], \\\n",
    "                      negative=[\"quality\"], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tablet â€“ Phone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec.wv.most_similar(positive=[\"tablet\"], \\\n",
    "                      negative=[\"phone\"], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named-Entity Recognition ###\n",
    "We've seen *gensim* perform word tagging to identify part-of-speech. Now we use *spaCy* to go further and identify what nouns in the documents refer to. Some Named-Entity Recognition (*NER*) classification tags include distinguishing persons, organizations, products, places, dates, etc.\n",
    "\n",
    "In exploring *spaCy*, we'll be using the `most_helpful_text`, which is the highest-rated product review by Amazon users. The `helpful` series from the `df` dataframe is actually a list with its first element storing the number of *helpful* votes a review received, and the second element containing the total number of *helpful* and *not helpful* review votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "helpful = df[\"helpful\"].tolist()\n",
    "most_helpful = max(helpful, key=lambda x: x[0])\n",
    "\n",
    "most_helpful_idx = df[\"helpful\"].astype(str) == str(most_helpful)\n",
    "most_helpful_idx = df[most_helpful_idx].index\n",
    "\n",
    "most_helpful_text = df[\"reviewText\"].iloc[most_helpful_idx].values[0]\n",
    "\n",
    "print(most_helpful_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `ner_dict`, a dictionary initialized as a list, to segregate the nouns in the `most_helpful_text` into the NER tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import spacy\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "ner = spacy.load(\"en\")\n",
    "\n",
    "ner_helpful = ner(most_helpful_text)\n",
    "\n",
    "ner_dict = defaultdict(list)\n",
    "for entity in ner_helpful.ents:\n",
    "    ner_dict[entity.label_].append(entity)\n",
    "\n",
    "for NER, name in ner_dict.items():\n",
    "    print(f\"{NER}:\\n{name}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `displaCy` to visualize the tags in the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(ner_helpful, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Tree ##\n",
    "The capability of *spaCy'*s NER is based on deciphering the structure of the sentence by breaking down how tokens interact with and influence each other. Below is the dependency trees of the first three sentences of the `most_helpful_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_displacy(sentence):\n",
    "    ner_sentence = ner(sentence)\n",
    "    displacy.render(ner_sentence, jupyter=True, \\\n",
    "                    options={\"compact\": False, \\\n",
    "                             \"distance\": 90, \\\n",
    "                             \"word_spacing\":20, \\\n",
    "                             \"arrow_spacing\":10, \\\n",
    "                             \"arrow_stroke\": 2, \\\n",
    "                             \"arrow_width\": 5})\n",
    "\n",
    "for sentence in most_helpful_text.split(\".\")[0:3]:\n",
    "    ner_displacy(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Latent Dirichlet Allocation (*LDA*) can cluster documents together according to topic, the reviews can be classified and grouped according to the type of electronics product they correspond to. The product reviews will have weights assigned to each of the topic and the topics themselves will have weights on every token. As it is a clustering-based model, LDA is unsupervised and only the `num_topics` is configurable.\n",
    "\n",
    "The following are the top five words that are salient to the first group of product reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import multiprocessing\n",
    "\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "num_topics = 10\n",
    "bow_lda = LdaMulticore(bow, num_topics=num_topics, id2word=vocabulary, \\\n",
    "                       passes=5, workers=cores, random_state=42)\n",
    "\n",
    "for token, frequency in bow_lda.show_topic(0, topn=5):\n",
    "    print(token, frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words that are the most characteristic of the topics are indeed thematic. And each word group do conjure a distinct topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in range(0, num_topics):\n",
    "    print(f\"\\nTopic {topic+1}:\")\n",
    "    for token, frequency in bow_lda.show_topic(topic, topn=5):\n",
    "        print(f\" {token}, {frequency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using *pyLDAvis*, we can interactively explore the words associated with the topics derived by LDA. The Intertopic Distance Map shows how some product reviews in one topic converge with others due to similarity. If needed, we can adjust the `num_topics` accordingly to cluster together topic intersections so a more evident decision boundary between classes can be established."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "lda_idm = pyLDAvis.gensim.prepare(bow_lda, bow, vocabulary)\n",
    "\n",
    "pyLDAvis.display(lda_idm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning #\n",
    "We'll further process our finalized dataframe in order to make it compatible and easy to pipe into our Machine Learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with NaNs ##\n",
    "It is important that we impute NaN values before we feed them into a model because machine learning algorithms can only work with *real* numbers. Our dataframe was derived from employing a *Word2Vec* model and so the only way we could have invalid entries that would become NaN values is when we have empty documents.\n",
    "\n",
    "If a review contains no tokens then every dimension would become NaN. And so to find out the indices of NaN documents, we just have to filter reviews that have a NaN on the first dimension (or any dimension at all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_list = model_df[model_df[0].isna()].index\n",
    "nan_list = nan_list.tolist()\n",
    "\n",
    "print(nan_list[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, inspecting these documents brings us empty lists which tell us that there are no tokens in the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*[tokenized[blank] for blank in nan_list[0:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imposing these indices to our `df`, we can extract what these reviews originally looked like before tokenization and before all the pre-processing steps were performed. We see that, other than blanks, reviews that would become NaNs contain only minimal characters. The fourth entry is invalidated because in our steps, we have dropped all characters that are not alphanumeric leaving us with just the letter *A*. We have also chosen in our pre-processing that single-characters would not be tokenized. The fourth review would therefore end up as an empty list after our NLP steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for blank in nan_list[0:5]:\n",
    "    display(df[\"reviewText\"].iloc[blank])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model_df` is updated by dropping the NaN documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Original 'model_df' count: {len(model_df)}\")\n",
    "print(f\"Final 'model_df' count: {len(model_df.dropna(axis=0))}\")\n",
    "\n",
    "model_df = model_df.dropna(axis=0)\n",
    "display(model_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Unbalanced Data ##\n",
    "The distribution of ratings shows that, in general, users highly approve of products bought on Amazon. This however gives us a highly imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "f, axes = plt.subplots(figsize=(20,7))\n",
    "ax = sns.countplot(x=df[\"overall\"], palette=\"OrRd_r\")\n",
    "ax.set(title=\"Distribution of Product Ratings\", \\\n",
    "       xlabel=\"Rating\", ylabel=\"Number of Reviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model simply classified every review as `5`, then an accuracy of around 60% can be achieved given this exact dataset. Since this would outperform predictions made by chance, we should therefore ensure that we stratify the testing set where we base the final score of the model.\n",
    "\n",
    "To deal with this we will have to take into account underrepresenting the majority and/or overrepresenting the minority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "majority = df[\"overall\"] == 5\n",
    "majority_ratio = len(df[majority]) / len(df)\n",
    "\n",
    "print(f\"{majority_ratio*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underrepresentation vs. Overrepresentation ##\n",
    "\n",
    "Performing over-representation is possible by bootstrapping the minority classes to match the size of the majority classes. This can be done using K-Nearest Neighbors (*KNN*) or via Support Vector Machine (*SVM*) by clustering a given class first before generating random samples within the decision boundaries of the class. A popular module called `SMOTE`, or *Synthetic Minority Over-sampling Technique*, does exactly this. However, since the imbalance in our classes is massive, and because we have 100 dimensions for each one of our almost 1.7 million observations, this approach is extremely computationally expensive.\n",
    "\n",
    "Because our dataset is huge, we can afford to perform sampling in every class and still have a significant amount of data for the model. This way, we can then opt to *underrepresent* the majority class according to our most minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = len(model_df[model_df[\"label\"] == 2])\n",
    "print(f\"Size of the most underrepresented class: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In choosing this route to deal with imbalance, we create a trimmed version of our dataframe, `trimmed_df`. Each class is trimmed to have the same number of entries as the smallest class which is *Class 2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trim the majority class\n",
    "condition = model_df[\"label\"] == 5\n",
    "trimmed_df = model_df[condition].sample(n=count, random_state=42)\n",
    "\n",
    "#trim other class and add on to the trimmed_df\n",
    "for rating in [1, 2, 3, 4]:\n",
    "    condition = model_df[\"label\"] == rating\n",
    "    if len(model_df[condition]) >= count:\n",
    "        add_df = model_df[condition].sample(n=count, random_state=42)\n",
    "    else:\n",
    "        add_df = model_df[condition]\n",
    "    trimmed_df = pd.concat([trimmed_df, add_df], ignore_index=False)\n",
    "\n",
    "#display new class sizes of trimmed_df\n",
    "for rating in [1, 2, 3, 4, 5]:\n",
    "    class_size = len(trimmed_df[trimmed_df[\"label\"] == rating])\n",
    "    print(f\"Size of Class {rating}: {class_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `trimmed_df` is arranged by class from 1 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trimmed_df = trimmed_df.sort_values(by=\"label\")\n",
    "display(trimmed_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we now have a perfectly balanced dataset after we performed underrepresentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(figsize=(20,7))\n",
    "ax = sns.countplot(x=trimmed_df[\"label\"], palette=\"OrRd_r\")\n",
    "ax.set(title=\"Distribution of Product Ratings after Underrepresentation\", \\\n",
    "       xlabel=\"Rating\", ylabel=\"Number of Reviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split ##\n",
    "The `y` is our target variable or the labels for the data. The `X` constitutes the features and are the predictor variables.\n",
    "\n",
    "We evenly split the training and testing sets and *stratify* to ensure the ratio of classes in both sets are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = trimmed_df.iloc[:, :-1]\n",
    "y = trimmed_df.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring and Baseline ##\n",
    "In our study, we will make use of two metrics to measure the model performance:\n",
    "* Accuracy\n",
    "* F1 Score\n",
    "\n",
    "Accuracy will identify how many reviews are correctly labeled by the model. There are five ratings and thus five classes. No review can have two or more ratings and so the probability that a correct prediction is made from pure guesswork is `20%`.\n",
    "\n",
    "The F1 score is taking *precision* and *recall* into consideration. Taking into account false positives and false negatives for each class is especially important in inherently imbalanced datasets.\n",
    "\n",
    "The baseline scores below are for when a model only randomly guesses the output labels â€“ in this case, when every prediction is the same class. The scores are also based on an evenly distributed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "label_shape = np.shape(y_test)\n",
    "y_baseline = np.full(label_shape, 5)\n",
    "\n",
    "accuracy_baseline = metrics.accuracy_score(y_test, y_baseline)\n",
    "f1_score_baseline = metrics.f1_score(y_test, y_baseline, average=\"micro\")\n",
    "\n",
    "print(f\"Baseline Accuracy: {accuracy_baseline*100:.3f}%\")\n",
    "print(f\"Baseline F1 Score: {f1_score_baseline:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest ##\n",
    "Random Forest actually has a native way of supporting datasets that have class imbalance. We will therefore be able to use the original `model_df` instead of the sample `trimmed_df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = model_df.iloc[:, :-1]\n",
    "y = model_df.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `class_weight` attribute is provided with a dictionary that represents the associated weight of each class â€“ the majority class is given a *1* and the rest are given the multiplying factor at which they would level with the largest class.\n",
    "\n",
    "The criteria chosen is `entropy` which is similar to `gini` but instead of splitting nodes until there are pure classes, the nodes are split until the classes within have equal probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=25, class_weight={1:10, 2:12, 3:7, 4:9, 5:1}, \\\n",
    "                                criterion=\"entropy\", random_state=42)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tuned Random Forest model got a very high score on the training data. The confusion matrix plotted below highlighted how the model almost perfectly classified each Amazon review accordingly.\n",
    "\n",
    "However, these scores may be misleading since they are based on the data that the model were trained on. This is highly likely a result of *overfitting*. It is then important to rate our model more effectively without digging into our reserved test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = forest.predict(X_train)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_train, y_pred)\n",
    "f1_score = metrics.f1_score(y_train, y_pred, average=\"micro\")\n",
    "\n",
    "print(f\"Training Set Accuracy: {accuracy*100:.3f}%\")\n",
    "print(f\"Training Set F1 Score: {f1_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#create the confusion matrix of the training set\n",
    "confusion_train = confusion_matrix(y_train, y_pred)\n",
    "confusion_train = confusion_train.astype(\"float\") / \\\n",
    "                   confusion_train.sum(axis=1)[:, np.newaxis]\n",
    "confusion_train = np.around(confusion_train, decimals=3)*100\n",
    "\n",
    "#create confusion matrix heat map\n",
    "f, axes = plt.subplots(figsize=(20,10))\n",
    "im = axes.imshow(confusion_train, interpolation=\"nearest\", cmap=plt.cm.Reds)\n",
    "\n",
    "axes.figure.colorbar(im, ax=axes)\n",
    "axes.set(title=\"Confusion Matrix for Training Set\", \\\n",
    "         xticks=np.arange(confusion_train.shape[1]), \\\n",
    "         yticks=np.arange(confusion_train.shape[0]), \\\n",
    "         xticklabels=range(1, 6), yticklabels=range(1, 6), \\\n",
    "         xlabel=\"Predicted\", ylabel=\"Truth\")\n",
    "\n",
    "#add clear annotations to the confusion matrix\n",
    "threshold = confusion_train.max()/1.5\n",
    "for i in range(confusion_train.shape[0]):\n",
    "    for j in range(confusion_train.shape[1]):\n",
    "        axes.text(j, i, f\"{confusion_train[i, j]:.3f}%\",\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if confusion_train[i, j] > threshold else \"black\")\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation ##\n",
    "Cross-validation makes the most of the training data by splitting the training set into *folds* and further subjecting each fold to train-test splits. Cross-validation can thus test against overfitting and the resulting scores can better reflect how the model performs on data it has not seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_accuracy = cross_val_score(forest, X_train, y_train, \\\n",
    "                               cv=3, scoring=\"accuracy\")\n",
    "cross_val_f1 = cross_val_score(forest, X_train, y_train, \\\n",
    "                               cv=3, scoring=\"f1_micro\")\n",
    "\n",
    "cross_val_accuracy = np.mean(cross_val_accuracy)\n",
    "cross_val_f1 = np.mean(cross_val_f1)\n",
    "\n",
    "print(f\"Training Set Accuracy: {cross_val_accuracy*100:.3f}%\")\n",
    "print(f\"Training Set F1 Score: {cross_val_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to create a model based on a popular boosting technique and see how it compares with our Random Forest model (which is a tree-based bagging approach). XGBoost has become a staple in Kaggle competitions because of its high rate of success and its ease-of-use.\n",
    "\n",
    "The class notation for our *XGBoost* object `boost` begins from 0, and so we perform an element-wise shift of our labels *from 1 to 0*, *from 2 to 1*, *from 3 to 2*, etc. We tune our model using the maximum number of depths, the learning rate (*eta*), the number of classes, etc. We expect our outputs to be multi-class and so we select `softprob` as our *objective*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import xgboost as xgb\n",
    "\n",
    "y_train_shifted = y_train-1\n",
    "y_test_shifted = y_test-1\n",
    "\n",
    "train_set = xgb.DMatrix(X_train, label=y_train_shifted)\n",
    "test_set = xgb.DMatrix(X_test, label=y_test_shifted)\n",
    "\n",
    "parameters = {\"max_depth\": 10, \"eta\": 0.2, \"silent\": 1, \\\n",
    "              \"objective\": \"multi:softprob\", \"num_class\": 5}\n",
    "\n",
    "boost = xgb.train(parameters, train_set, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array of predicted labels `y_pred` contains lists of probabilities for each class per product review. The class that is deemed most likely is chosen by the *argmax* and the labels are shifted back to their original state.\n",
    "\n",
    "The `micro` approach in averaging the F1 score means that the false positives, true positives, and false negatives are taken into account across all classes. This is in contrast with the `macro` approach that instead averages the F1 scores of each class independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = boost.predict(train_set)\n",
    "y_pred = y_pred.argmax(axis=1)\n",
    "y_pred = y_pred+1\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_train, y_pred)\n",
    "f1_score = metrics.f1_score(y_train, y_pred, average=\"micro\")\n",
    "\n",
    "print(f\"Training Set Accuracy: {accuracy*100:.3f}%\")\n",
    "print(f\"Training Set F1 Score: {f1_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the confusion matrix of the training set\n",
    "confusion_train = confusion_matrix(y_train, y_pred)\n",
    "confusion_train = confusion_train.astype(\"float\") / \\\n",
    "                   confusion_train.sum(axis=1)[:, np.newaxis]\n",
    "confusion_train = np.around(confusion_train, decimals=3)*100\n",
    "\n",
    "#create confusion matrix heat map\n",
    "f, axes = plt.subplots(figsize=(20,10))\n",
    "im = axes.imshow(confusion_train, interpolation=\"nearest\", cmap=plt.cm.Reds)\n",
    "\n",
    "axes.figure.colorbar(im, ax=axes)\n",
    "axes.set(title=\"Confusion Matrix for Training Set\", \\\n",
    "         xticks=np.arange(confusion_train.shape[1]), \\\n",
    "         yticks=np.arange(confusion_train.shape[0]), \\\n",
    "         xticklabels=range(1, 6), yticklabels=range(1, 6), \\\n",
    "         xlabel=\"Predicted\", ylabel=\"Truth\")\n",
    "\n",
    "#add clear annotations to the confusion matrix\n",
    "threshold = confusion_train.max()/1.5\n",
    "for i in range(confusion_train.shape[0]):\n",
    "    for j in range(confusion_train.shape[1]):\n",
    "        axes.text(j, i, f\"{confusion_train[i, j]:.3f}%\",\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if confusion_train[i, j] > threshold else \"black\")\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fairly compare our boosting results with our Random Forest outcome, we perform cross-validation on three folds of the training data set as well.\n",
    "\n",
    "However, since the XGBoost implementation we used is not supported by *scikit-learn*'s `.fit` method, the cross-validation must be done using `xgboost`'s own API. The output `boost_cv` is actually a *pandas* dataframe that tabulates the results of the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "boost_cv = xgb.cv(dtrain=train_set, params=parameters, nfold=3, \\\n",
    "                  num_boost_round=50, early_stopping_rounds=10, \\\n",
    "                  metrics=\"merror\", as_pandas=True, seed=42)\n",
    "\n",
    "display(boost_cv.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the training set cross-validation score by getting the *merror* mean on the 50th `num_boost_round`, which is the final boosting phase. The *merror* is an accuracy error rate metric meant for multi-class labels.\n",
    "\n",
    "We can get a sense of how accurate the model is by subtracting the *merror* value from a perfect score of 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_accuracy = boost_cv.iloc[-1,2]\n",
    "cross_val_accuracy = 1-cross_val_accuracy\n",
    "\n",
    "print(f\"Training Set Accuracy: {cross_val_accuracy*100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Scores ##\n",
    "Seeing that the boosting model outperformed the Random Forest approach in the three-fold cross validation, we can now apply our model on the testing set that we have put aside early on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = boost.predict(test_set)\n",
    "y_pred = y_pred.argmax(axis=1)\n",
    "y_pred = y_pred+1\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "f1_score = metrics.f1_score(y_test, y_pred, average=\"micro\")\n",
    "\n",
    "print(f\"Test Set Accuracy: {accuracy*100:.3f}%\")\n",
    "print(f\"Test Set F1 Score: {f1_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the confusion matrix of the test set\n",
    "confusion_train = confusion_matrix(y_test, y_pred)\n",
    "confusion_train = confusion_train.astype(\"float\") / \\\n",
    "                   confusion_train.sum(axis=1)[:, np.newaxis]\n",
    "confusion_train = np.around(confusion_train, decimals=3)*100\n",
    "\n",
    "#create confusion matrix heat map\n",
    "f, axes = plt.subplots(figsize=(20,10))\n",
    "im = axes.imshow(confusion_train, interpolation=\"nearest\", cmap=plt.cm.Reds)\n",
    "\n",
    "axes.figure.colorbar(im, ax=axes)\n",
    "axes.set(title=\"Confusion Matrix for Test Set\", \\\n",
    "         xticks=np.arange(confusion_train.shape[1]), \\\n",
    "         yticks=np.arange(confusion_train.shape[0]), \\\n",
    "         xticklabels=range(1, 6), yticklabels=range(1, 6), \\\n",
    "         xlabel=\"Predicted\", ylabel=\"Truth\")\n",
    "\n",
    "#add clear annotations to the confusion matrix\n",
    "threshold = confusion_train.max()/1.5\n",
    "for i in range(confusion_train.shape[0]):\n",
    "    for j in range(confusion_train.shape[1]):\n",
    "        axes.text(j, i, f\"{confusion_train[i, j]:.3f}%\",\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if confusion_train[i, j] > threshold else \"black\")\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our results above were actually based on the original `model_df` dataset that had the massive class imbalance. Let's now reassign our `X` and `y` variables to the balanced `trimmed_df` sample dataset we've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trimmed_df.iloc[:, :-1]\n",
    "y = trimmed_df.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_shifted = y_train-1\n",
    "y_test_shifted = y_test-1\n",
    "\n",
    "train_set = xgb.DMatrix(X_train, label=y_train_shifted)\n",
    "test_set = xgb.DMatrix(X_test, label=y_test_shifted)\n",
    "\n",
    "y_pred = boost.predict(test_set)\n",
    "y_pred = y_pred.argmax(axis=1)\n",
    "y_pred = y_pred+1\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "f1_score = metrics.f1_score(y_test, y_pred, average=\"micro\")\n",
    "\n",
    "print(f\"Balanced Test Set Accuracy: {accuracy*100:.3f}%\")\n",
    "print(f\"Balanced Test Set F1 Score: {f1_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the confusion matrix of the balanced test set\n",
    "confusion_train = confusion_matrix(y_test, y_pred)\n",
    "confusion_train = confusion_train.astype(\"float\") / \\\n",
    "                   confusion_train.sum(axis=1)[:, np.newaxis]\n",
    "confusion_train = np.around(confusion_train, decimals=3)*100\n",
    "\n",
    "#create confusion matrix heat map\n",
    "f, axes = plt.subplots(figsize=(20,10))\n",
    "im = axes.imshow(confusion_train, interpolation=\"nearest\", cmap=plt.cm.Reds)\n",
    "\n",
    "axes.figure.colorbar(im, ax=axes)\n",
    "axes.set(title=\"Confusion Matrix for Balanced Test Set\", \\\n",
    "         xticks=np.arange(confusion_train.shape[1]), \\\n",
    "         yticks=np.arange(confusion_train.shape[0]), \\\n",
    "         xticklabels=range(1, 6), yticklabels=range(1, 6), \\\n",
    "         xlabel=\"Predicted\", ylabel=\"Truth\")\n",
    "\n",
    "#add clear annotations to the confusion matrix\n",
    "threshold = confusion_train.max()/1.5\n",
    "for i in range(confusion_train.shape[0]):\n",
    "    for j in range(confusion_train.shape[1]):\n",
    "        axes.text(j, i, f\"{confusion_train[i, j]:.3f}%\",\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if confusion_train[i, j] > threshold else \"black\")\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 53.3% on a perfectly balanced training data set, we have achieved a better result compared to the 20% accuracy of our baseline.\n",
    "\n",
    "## Word Cloud ##\n",
    "Using the true labels of the reviews, we can take the fifty most salient words in every rating and produce a word cloud. The same `stop_words` we derived from the NLTK library are excluded.\n",
    "\n",
    "We see that some of the words are quite descriptive of the rating, with \"problem\" and \"issue\" frequently appearing in one-star reviews, and \"quality\" and \"highly recommend\" in top reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "wordcloud = WordCloud(stopwords = set(stop_words), min_font_size=10, \\\n",
    "                      max_font_size=50, max_words=50, \\\n",
    "                      background_color=\"white\", colormap = \"Oranges\")\n",
    "\n",
    "one_star_text = \" \".join(df[df[\"Positively_Rated\"]==0][\"reviewText\"].values).lower()\n",
    "two_star_text = \" \".join(df[df[\"Positively_Rated\"]==1][\"reviewText\"].values).lower()\n",
    "#three_star_text = \" \".join(df[df[\"overall\"]==3][\"reviewText\"].values).lower()\n",
    "#four_star_text = \" \".join(df[df[\"overall\"]==4][\"reviewText\"].values).lower()\n",
    "#five_star_text = \" \".join(df[df[\"overall\"]==5][\"reviewText\"].values).lower()\n",
    "\n",
    "text_list = [one_star_text, two_star_text]\n",
    "\n",
    "for index, text in enumerate(text_list):\n",
    "    f, axes = plt.subplots(figsize=(10,7))\n",
    "    wordcloud.generate(text)\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.title(f\"Word Cloud for {index+1}-Star Ratings\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion #\n",
    "A lot of Natural Language Processing techniques were covered in the study. Just some of the concepts explored include topic modeling â€“ where similar texts were clustered together according to topic, named entity recognition (NER) â€“ where nouns were given identifying labels like *place* or *time*, and dependency trees â€“ where parts-of-speech tags and sentence structure were discerned. Though the *Word2Vec* phase was central to our final model, the pre-processing steps were perhaps just as crucial. Prior to tokenization, each document had to be decoded from UTF and encoded to ASCII, and converted to lowercase. The texts were stripped of accents, stop words and punctuation, and multiple whitespaces were dropped. Words were simplified to their root words in order to compact the vocabulary as much as possible. Tokens that were often used together were also singularized through phrase modeling.\n",
    "\n",
    "Beyond word use and word frequency, our model actually extracts and quantifies *context*. Every token in all the reviews are understood by their neighboring words and embedded in a given number of dimensions. All the interactions of a word with all the other words it has been associated with are expressed in vectors. And all the words in a given review are averaged according to each of the dimensions to create its 100 features. So the essence of a review by its words make up the final dataframe.\n",
    "\n",
    "What we have is a multi-class model where each of the five classes correspond to a reviewâ€™s star rating. This is then a discrete approach where each class is independent of each other. In a situation where a 5-star rating is misinterpreted by the model as a 1-star review, then the model has simply misclassified â€“ it is agnostic to how far off `1` and `5` are. This is in contrast with a *continuous* approach whereas a misclassification of a 5-star review as a 1-star review would be more penalizing. Our model then is reliant on the distinction of each kind of review. It is more concerned in asking \"*What makes a 5-star review different from a 4-star review?*\" than asking \"*Is this review more approving than criticizing?*\"\n",
    "\n",
    "## Limitations and Recommendations ##\n",
    "Though we have observed satisfactory results in our model compared to the baseline, there are several limitations in the way the model handles data. These could serve as areas of improvement. First, despite a rich vocabulary, the model will not be able to handle words that it has not encountered during training. In fact, if an unknown word appears in a review, the word is dropped from the dimension-averaging step since has not  been referenced in our `word_vec_df`.\n",
    "\n",
    "Because each word is simplified by lemmatization during pre-processing, then alternate forms of a token shouldnâ€™t necessarily be a concern. However, the model cannot identify if a word is misspelled and will identify one simply as a new word. Incorporating a spellchecker would add to the computational cost and will certainly add to the modelâ€™s complexity.\n",
    "\n",
    "Finally, as is usually the case in NLP, sarcasm or text that is intended to be ironic is interpreted by what is literally in the text and not by its underlying context. Because sarcasm is usually detected by readers through the mood and sentiment of the document, it takes adding another layer of NLP just to approximate whether the review is sarcastic or not in order to properly work with such text. This supplement layer will not only utilize tagged sarcastic text as supervised labels, but must also consider the reviewâ€™s given product rating in its judgment to detect sarcasm."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
